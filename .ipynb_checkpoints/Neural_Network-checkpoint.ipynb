{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import random\n",
    "from sklearn.model_selection import train_test_split  \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Installing Theano\n",
    "# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n",
    "\n",
    "# Installing Tensorflow\n",
    "# Install Tensorflow from the website: https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html\n",
    "\n",
    "# Installing Keras\n",
    "# pip install --upgrade keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r pca_train_x\n",
    "%store -r pca_test_x\n",
    "\n",
    "%store -r train_x\n",
    "%store -r test_x\n",
    "%store -r train_y\n",
    "%store -r test_y\n",
    "\n",
    "%store -r train_x_two_features\n",
    "%store -r test_x_two_features\n",
    "%store -r train_y_two_features\n",
    "%store -r test_y_two_features\n",
    "\n",
    "%store -r pca_train_x\n",
    "%store -r pca_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START: OWN CODE\n",
    "# Learning Parameters\n",
    "rate   =   0.0010  # training rate\n",
    "epochs =  200     # number of full training cycles \n",
    "banch_size  = 68   # number of data points to train per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_hidden_1 = 512  # number of nodes in hidden layer 1\n",
    "n_hidden_2 = 512  # number of nodes in hidden layer 2\n",
    "n_input    = train_x.shape[-1]  # 20\n",
    "n_classes  = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder('float', [None,n_input])\n",
    "Y = tf.placeholder('float', [None,n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'w1' : tf.Variable(tf.random_normal([n_input,    n_hidden_1])),\n",
    "    'w2' : tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes ]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1' : tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2' : tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes ]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(X, weights, biases):  \n",
    "    # Hidden Layer 1\n",
    "    layer1 = tf.matmul(X, weights['w1'])\n",
    "    layer1 = tf.add(layer1, biases['b1'])\n",
    "    layer1 = tf.nn.softmax(layer1)\n",
    "    \n",
    "    # Hidden Layer 2\n",
    "    layer2 = tf.matmul(layer1, weights['w2'])\n",
    "    layer2 = tf.add(layer2, biases['b2'])\n",
    "    layer2 = tf.nn.softmax(layer2)\n",
    "    \n",
    "    # Output Layer\n",
    "    output = tf.matmul(layer2, weights['out'])\n",
    "    output = tf.add(output, biases['out'])\n",
    "    output = tf.nn.softmax(output)\n",
    "    \n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section we define the optimizer and loss functions. \n",
    "# We also define our notion of correct and incorrect prediction and what we mean by accuracy.\n",
    "\n",
    "def run_my_neural_network(train_x,train_y,test_x,test_y):\n",
    "    train_y = train_y.reshape(len(train_y), 1)\n",
    "    test_y = test_y.reshape(len(test_y), 1)\n",
    "\n",
    "    # Classification: label(gender) = {male, female} → [1.0, 0.0], [0.0, 1.0]  \n",
    "    labels_tmp = []  \n",
    "    for label in train_y:  \n",
    "        tmp = []  \n",
    "        if label[0] == 0:  \n",
    "            tmp = [0.0, 1.0]  \n",
    "        else:   \n",
    "            tmp = [1.0, 0.0]  \n",
    "        labels_tmp.append(tmp)  \n",
    "    train_y = np.array(labels_tmp) \n",
    "    \n",
    "    \n",
    "    \n",
    "    labels_tmp = []  \n",
    "    for label in test_y:  \n",
    "        tmp = []  \n",
    "        if label[0] == 0:  \n",
    "            tmp = [0.0, 1.0]  \n",
    "        else:   \n",
    "            tmp = [1.0, 0.0]  \n",
    "        labels_tmp.append(tmp)  \n",
    "    test_y = np.array(labels_tmp) \n",
    "    \n",
    "    model = neural_network(X, weights, biases)\n",
    "    f_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "    f_optimizer = tf.train.AdamOptimizer(learning_rate=rate).minimize(f_cost)\n",
    "    with tf.Session() as s:\n",
    "        s.run(tf.global_variables_initializer())\n",
    "        for epoch in range(epochs):\n",
    "            cost_avg = 0.\n",
    "            batch_total = len(train_x) // banch_size\n",
    "            for banch in range(batch_total):\n",
    "                voice_banch = train_x[banch*banch_size:(banch+1)*(banch_size)]  \n",
    "                label_banch = train_y[banch*banch_size:(banch+1)*(banch_size)]        \n",
    "                _, cost = s.run([f_optimizer,f_cost], feed_dict={X:voice_banch, Y: label_banch})        \n",
    "                cost_avg += cost / batch_total\n",
    "        \n",
    "            print('Epoch {}: cost={:.4f}'.format(epoch+1, cost_avg))\n",
    "    \n",
    "        # testing\n",
    "        \n",
    "        # This gives us a list of booleans\n",
    "        prediction = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))  \n",
    "        # We cast to floating point numbers and then take the mean\n",
    "        accuracy = tf.reduce_mean(tf.cast(prediction, dtype=tf.float32))  \n",
    "        accuracy = s.run(accuracy, feed_dict={X: train_x, Y: train_y})  \n",
    "        print('In-sample accuracy in Neural Network: %s'  % (accuracy)) \n",
    "        \n",
    "        accuracy = tf.reduce_mean(tf.cast(prediction, dtype=tf.float32)) \n",
    "        accuracy = s.run(accuracy, feed_dict={X: test_x, Y: test_y})  \n",
    "        print('Out-of-sample accuracy in Neural Network: %s'  % (accuracy)) \n",
    "\n",
    "# END: OWN CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: cost=0.8012\n",
      "Epoch 2: cost=0.7955\n",
      "Epoch 3: cost=0.7880\n",
      "Epoch 4: cost=0.7778\n",
      "Epoch 5: cost=0.7637\n",
      "Epoch 6: cost=0.7444\n",
      "Epoch 7: cost=0.7183\n",
      "Epoch 8: cost=0.6840\n",
      "Epoch 9: cost=0.6441\n",
      "Epoch 10: cost=0.6043\n",
      "Epoch 11: cost=0.5678\n",
      "Epoch 12: cost=0.5359\n",
      "Epoch 13: cost=0.5085\n",
      "Epoch 14: cost=0.4854\n",
      "Epoch 15: cost=0.4660\n",
      "Epoch 16: cost=0.4496\n",
      "Epoch 17: cost=0.4357\n",
      "Epoch 18: cost=0.4240\n",
      "Epoch 19: cost=0.4139\n",
      "Epoch 20: cost=0.4053\n",
      "Epoch 21: cost=0.3978\n",
      "Epoch 22: cost=0.3912\n",
      "Epoch 23: cost=0.3855\n",
      "Epoch 24: cost=0.3803\n",
      "Epoch 25: cost=0.3758\n",
      "Epoch 26: cost=0.3717\n",
      "Epoch 27: cost=0.3681\n",
      "Epoch 28: cost=0.3648\n",
      "Epoch 29: cost=0.3618\n",
      "Epoch 30: cost=0.3590\n",
      "Epoch 31: cost=0.3565\n",
      "Epoch 32: cost=0.3542\n",
      "Epoch 33: cost=0.3521\n",
      "Epoch 34: cost=0.3501\n",
      "Epoch 35: cost=0.3483\n",
      "Epoch 36: cost=0.3467\n",
      "Epoch 37: cost=0.3451\n",
      "Epoch 38: cost=0.3437\n",
      "Epoch 39: cost=0.3424\n",
      "Epoch 40: cost=0.3411\n",
      "Epoch 41: cost=0.3400\n",
      "Epoch 42: cost=0.3388\n",
      "Epoch 43: cost=0.3378\n",
      "Epoch 44: cost=0.3368\n",
      "Epoch 45: cost=0.3359\n",
      "Epoch 46: cost=0.3351\n",
      "Epoch 47: cost=0.3343\n",
      "Epoch 48: cost=0.3336\n",
      "Epoch 49: cost=0.3329\n",
      "Epoch 50: cost=0.3323\n",
      "Epoch 51: cost=0.3317\n",
      "Epoch 52: cost=0.3312\n",
      "Epoch 53: cost=0.3307\n",
      "Epoch 54: cost=0.3302\n",
      "Epoch 55: cost=0.3298\n",
      "Epoch 56: cost=0.3293\n",
      "Epoch 57: cost=0.3290\n",
      "Epoch 58: cost=0.3286\n",
      "Epoch 59: cost=0.3283\n",
      "Epoch 60: cost=0.3279\n",
      "Epoch 61: cost=0.3276\n",
      "Epoch 62: cost=0.3274\n",
      "Epoch 63: cost=0.3271\n",
      "Epoch 64: cost=0.3268\n",
      "Epoch 65: cost=0.3266\n",
      "Epoch 66: cost=0.3263\n",
      "Epoch 67: cost=0.3261\n",
      "Epoch 68: cost=0.3259\n",
      "Epoch 69: cost=0.3257\n",
      "Epoch 70: cost=0.3255\n",
      "Epoch 71: cost=0.3253\n",
      "Epoch 72: cost=0.3251\n",
      "Epoch 73: cost=0.3250\n",
      "Epoch 74: cost=0.3248\n",
      "Epoch 75: cost=0.3246\n",
      "Epoch 76: cost=0.3245\n",
      "Epoch 77: cost=0.3243\n",
      "Epoch 78: cost=0.3242\n",
      "Epoch 79: cost=0.3240\n",
      "Epoch 80: cost=0.3239\n",
      "Epoch 81: cost=0.3237\n",
      "Epoch 82: cost=0.3236\n",
      "Epoch 83: cost=0.3235\n",
      "Epoch 84: cost=0.3233\n",
      "Epoch 85: cost=0.3232\n",
      "Epoch 86: cost=0.3231\n",
      "Epoch 87: cost=0.3230\n",
      "Epoch 88: cost=0.3229\n",
      "Epoch 89: cost=0.3228\n",
      "Epoch 90: cost=0.3227\n",
      "Epoch 91: cost=0.3226\n",
      "Epoch 92: cost=0.3226\n",
      "Epoch 93: cost=0.3225\n",
      "Epoch 94: cost=0.3224\n",
      "Epoch 95: cost=0.3223\n",
      "Epoch 96: cost=0.3223\n",
      "Epoch 97: cost=0.3222\n",
      "Epoch 98: cost=0.3221\n",
      "Epoch 99: cost=0.3221\n",
      "Epoch 100: cost=0.3220\n",
      "Epoch 101: cost=0.3220\n",
      "Epoch 102: cost=0.3219\n",
      "Epoch 103: cost=0.3219\n",
      "Epoch 104: cost=0.3218\n",
      "Epoch 105: cost=0.3218\n",
      "Epoch 106: cost=0.3217\n",
      "Epoch 107: cost=0.3217\n",
      "Epoch 108: cost=0.3216\n",
      "Epoch 109: cost=0.3216\n",
      "Epoch 110: cost=0.3215\n",
      "Epoch 111: cost=0.3215\n",
      "Epoch 112: cost=0.3215\n",
      "Epoch 113: cost=0.3214\n",
      "Epoch 114: cost=0.3214\n",
      "Epoch 115: cost=0.3214\n",
      "Epoch 116: cost=0.3213\n",
      "Epoch 117: cost=0.3213\n",
      "Epoch 118: cost=0.3213\n",
      "Epoch 119: cost=0.3212\n",
      "Epoch 120: cost=0.3212\n",
      "Epoch 121: cost=0.3212\n",
      "Epoch 122: cost=0.3211\n",
      "Epoch 123: cost=0.3211\n",
      "Epoch 124: cost=0.3211\n",
      "Epoch 125: cost=0.3211\n",
      "Epoch 126: cost=0.3210\n",
      "Epoch 127: cost=0.3210\n",
      "Epoch 128: cost=0.3210\n",
      "Epoch 129: cost=0.3210\n",
      "Epoch 130: cost=0.3210\n",
      "Epoch 131: cost=0.3209\n",
      "Epoch 132: cost=0.3209\n",
      "Epoch 133: cost=0.3209\n",
      "Epoch 134: cost=0.3209\n",
      "Epoch 135: cost=0.3209\n",
      "Epoch 136: cost=0.3208\n",
      "Epoch 137: cost=0.3208\n",
      "Epoch 138: cost=0.3208\n",
      "Epoch 139: cost=0.3208\n",
      "Epoch 140: cost=0.3208\n",
      "Epoch 141: cost=0.3208\n",
      "Epoch 142: cost=0.3208\n",
      "Epoch 143: cost=0.3207\n",
      "Epoch 144: cost=0.3207\n",
      "Epoch 145: cost=0.3207\n",
      "Epoch 146: cost=0.3207\n",
      "Epoch 147: cost=0.3207\n",
      "Epoch 148: cost=0.3207\n",
      "Epoch 149: cost=0.3207\n",
      "Epoch 150: cost=0.3206\n",
      "Epoch 151: cost=0.3206\n",
      "Epoch 152: cost=0.3206\n",
      "Epoch 153: cost=0.3206\n",
      "Epoch 154: cost=0.3206\n",
      "Epoch 155: cost=0.3206\n",
      "Epoch 156: cost=0.3206\n",
      "Epoch 157: cost=0.3206\n",
      "Epoch 158: cost=0.3206\n",
      "Epoch 159: cost=0.3206\n",
      "Epoch 160: cost=0.3205\n",
      "Epoch 161: cost=0.3205\n",
      "Epoch 162: cost=0.3205\n",
      "Epoch 163: cost=0.3205\n",
      "Epoch 164: cost=0.3205\n",
      "Epoch 165: cost=0.3205\n",
      "Epoch 166: cost=0.3205\n",
      "Epoch 167: cost=0.3205\n",
      "Epoch 168: cost=0.3205\n",
      "Epoch 169: cost=0.3205\n",
      "Epoch 170: cost=0.3205\n",
      "Epoch 171: cost=0.3205\n",
      "Epoch 172: cost=0.3205\n",
      "Epoch 173: cost=0.3205\n",
      "Epoch 174: cost=0.3204\n",
      "Epoch 175: cost=0.3204\n",
      "Epoch 176: cost=0.3204\n",
      "Epoch 177: cost=0.3204\n",
      "Epoch 178: cost=0.3204\n",
      "Epoch 179: cost=0.3204\n",
      "Epoch 180: cost=0.3204\n",
      "Epoch 181: cost=0.3204\n",
      "Epoch 182: cost=0.3204\n",
      "Epoch 183: cost=0.3204\n",
      "Epoch 184: cost=0.3204\n",
      "Epoch 185: cost=0.3204\n",
      "Epoch 186: cost=0.3204\n",
      "Epoch 187: cost=0.3204\n",
      "Epoch 188: cost=0.3204\n",
      "Epoch 189: cost=0.3204\n",
      "Epoch 190: cost=0.3204\n",
      "Epoch 191: cost=0.3204\n",
      "Epoch 192: cost=0.3204\n",
      "Epoch 193: cost=0.3204\n",
      "Epoch 194: cost=0.3204\n",
      "Epoch 195: cost=0.3203\n",
      "Epoch 196: cost=0.3203\n",
      "Epoch 197: cost=0.3203\n",
      "Epoch 198: cost=0.3203\n",
      "Epoch 199: cost=0.3203\n",
      "Epoch 200: cost=0.3203\n",
      "In-sample accuracy in Neural Network: 0.992845\n",
      "Out-of-sample accuracy in Neural Network: 0.97601\n"
     ]
    }
   ],
   "source": [
    "run_my_neural_network(train_x,train_y,test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Keras deep learning library with a TensorFlow backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def run_neural_network(train_x,train_y,test_x,test_y):\n",
    "    \n",
    "    train_y = train_y.reshape(len(train_y), 1)\n",
    "    test_y = test_y.reshape(len(test_y), 1)\n",
    "    # Classification: label(gender) = {male, female} → [1.0, 0.0], [0.0, 1.0]  \n",
    "    labels_tmp = []  \n",
    "    for label in train_y:  \n",
    "        tmp = []  \n",
    "        if label[0] == 0:  \n",
    "            tmp = [0.0, 1.0]  \n",
    "        else:   \n",
    "            tmp = [1.0, 0.0]  \n",
    "        labels_tmp.append(tmp) \n",
    "    train_y = np.array(labels_tmp) \n",
    "\n",
    "    labels_tmp = []  \n",
    "    for label in test_y:  \n",
    "        tmp = []  \n",
    "        if label[0] == 0:  \n",
    "            tmp = [0.0, 1.0]  \n",
    "        else:   \n",
    "            tmp = [1.0, 0.0]  \n",
    "        labels_tmp.append(tmp)  \n",
    "    test_y = np.array(labels_tmp) \n",
    "    \n",
    "    \n",
    "    #Initialisng the ANN\n",
    "    classifier = Sequential()\n",
    "    #Input Layer and First Hidden Layer\n",
    "    classifier.add(Dense(units = 512, activation = 'softmax', input_dim = train_x.shape[1]))\n",
    "    #Adding the Second hidden layer\n",
    "    classifier.add(Dense(units = 512, activation = 'softmax'))\n",
    "    # Adding the output layer\n",
    "    classifier.add(Dense(units = 2, activation = 'softmax'))\n",
    "    #Compiling the ANN\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    #Fitting ANN to the training set\n",
    "    classifier.fit(train_x, train_y, batch_size = 68, epochs = 200, verbose = 0)\n",
    "    pred_train_y = classifier.predict(train_x)\n",
    "    pred_train_y = (pred_train_y > 0.5)\n",
    "    labels_tmp = []  \n",
    "    for label in pred_train_y:  \n",
    "        tmp = []  \n",
    "        if label[0] == True:  \n",
    "            tmp = [1,0]  \n",
    "        else:   \n",
    "            tmp = [0,1]  \n",
    "        labels_tmp.append(tmp)  \n",
    "    new_result = np.array(labels_tmp) \n",
    "    print('In-sample accuracy in Neural Network using Keras package (with all features):%s' % (accuracy_score(train_y, new_result)))\n",
    "   \n",
    "    # Predicting the Test set results\n",
    "    pred_y = classifier.predict(test_x)\n",
    "    pred_y = (pred_y > 0.5)\n",
    "    labels_tmp = []  \n",
    "    for label in pred_y:  \n",
    "        tmp = []  \n",
    "        if label[0] == True:  \n",
    "            tmp = [1,0]  \n",
    "        else:   \n",
    "            tmp = [0,1]  \n",
    "        labels_tmp.append(tmp)  \n",
    "    new_result = np.array(labels_tmp) \n",
    "    print('Out-of-sample accuracy in Neural Network using Keras package (with 2 features):%s' % (accuracy_score(test_y, new_result)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample accuracy in Neural Network using Keras package (with all features):0.997895622896\n",
      "Out-of-sample accuracy in Neural Network using Keras package (with 2 features):0.974747474747\n"
     ]
    }
   ],
   "source": [
    "run_neural_network(train_x,train_y,test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
