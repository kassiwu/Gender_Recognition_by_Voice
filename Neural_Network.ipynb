{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import random\n",
    "from sklearn.model_selection import train_test_split  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Installing Theano\n",
    "# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n",
    "\n",
    "# Installing Tensorflow\n",
    "# Install Tensorflow from the website: https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html\n",
    "\n",
    "# Installing Keras\n",
    "# pip install --upgrade keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -r pca_train_x\n",
    "%store -r pca_test_x\n",
    "%store -r pca13_train_x\n",
    "%store -r pca13_test_x\n",
    "\n",
    "%store -r train_x\n",
    "%store -r test_x\n",
    "%store -r train_y\n",
    "%store -r test_y\n",
    "\n",
    "%store -r train_x_two_features\n",
    "%store -r test_x_two_features\n",
    "%store -r train_y_two_features\n",
    "%store -r test_y_two_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2376/2376 [==============================] - 1s 408us/step - loss: 0.5878 - acc: 0.7782\n",
      "Epoch 2/100\n",
      "2376/2376 [==============================] - 1s 234us/step - loss: 0.2625 - acc: 0.8969\n",
      "Epoch 3/100\n",
      "2376/2376 [==============================] - 1s 243us/step - loss: 0.1463 - acc: 0.9541\n",
      "Epoch 4/100\n",
      "2376/2376 [==============================] - 1s 231us/step - loss: 0.1015 - acc: 0.9722\n",
      "Epoch 5/100\n",
      "2376/2376 [==============================] - 1s 220us/step - loss: 0.0878 - acc: 0.9722\n",
      "Epoch 6/100\n",
      "2376/2376 [==============================] - 1s 229us/step - loss: 0.0824 - acc: 0.9752\n",
      "Epoch 7/100\n",
      "2376/2376 [==============================] - 1s 227us/step - loss: 0.0794 - acc: 0.9752\n",
      "Epoch 8/100\n",
      "2376/2376 [==============================] - 1s 230us/step - loss: 0.0768 - acc: 0.9769\n",
      "Epoch 9/100\n",
      "2376/2376 [==============================] - 1s 225us/step - loss: 0.0763 - acc: 0.9752\n",
      "Epoch 10/100\n",
      "2376/2376 [==============================] - 1s 240us/step - loss: 0.0740 - acc: 0.9781\n",
      "Epoch 11/100\n",
      "2376/2376 [==============================] - 1s 273us/step - loss: 0.0731 - acc: 0.9781\n",
      "Epoch 12/100\n",
      "2376/2376 [==============================] - 1s 243us/step - loss: 0.0717 - acc: 0.9798\n",
      "Epoch 13/100\n",
      "2376/2376 [==============================] - 1s 259us/step - loss: 0.0700 - acc: 0.9781\n",
      "Epoch 14/100\n",
      "2376/2376 [==============================] - 1s 268us/step - loss: 0.0697 - acc: 0.9777\n",
      "Epoch 15/100\n",
      "2376/2376 [==============================] - 1s 298us/step - loss: 0.0687 - acc: 0.9802\n",
      "Epoch 16/100\n",
      "2376/2376 [==============================] - 1s 285us/step - loss: 0.0675 - acc: 0.9781\n",
      "Epoch 17/100\n",
      "2376/2376 [==============================] - 1s 233us/step - loss: 0.0668 - acc: 0.9806\n",
      "Epoch 18/100\n",
      "2376/2376 [==============================] - 1s 219us/step - loss: 0.0664 - acc: 0.9798\n",
      "Epoch 19/100\n",
      "2376/2376 [==============================] - 1s 219us/step - loss: 0.0649 - acc: 0.9794\n",
      "Epoch 20/100\n",
      "2376/2376 [==============================] - 1s 212us/step - loss: 0.0635 - acc: 0.9806\n",
      "Epoch 21/100\n",
      "2376/2376 [==============================] - 1s 228us/step - loss: 0.0653 - acc: 0.9806\n",
      "Epoch 22/100\n",
      "2376/2376 [==============================] - 1s 228us/step - loss: 0.0633 - acc: 0.9811\n",
      "Epoch 23/100\n",
      "2376/2376 [==============================] - 0s 210us/step - loss: 0.0629 - acc: 0.9811\n",
      "Epoch 24/100\n",
      "2376/2376 [==============================] - 1s 215us/step - loss: 0.0619 - acc: 0.9823\n",
      "Epoch 25/100\n",
      "2376/2376 [==============================] - 1s 232us/step - loss: 0.0614 - acc: 0.9802\n",
      "Epoch 26/100\n",
      "2376/2376 [==============================] - 1s 230us/step - loss: 0.0605 - acc: 0.9815\n",
      "Epoch 27/100\n",
      "2376/2376 [==============================] - 1s 215us/step - loss: 0.0604 - acc: 0.9815\n",
      "Epoch 28/100\n",
      "2376/2376 [==============================] - 1s 220us/step - loss: 0.0592 - acc: 0.9823\n",
      "Epoch 29/100\n",
      "2376/2376 [==============================] - 1s 228us/step - loss: 0.0592 - acc: 0.9832\n",
      "Epoch 30/100\n",
      "2376/2376 [==============================] - 1s 219us/step - loss: 0.0578 - acc: 0.9827\n",
      "Epoch 31/100\n",
      "2376/2376 [==============================] - 1s 229us/step - loss: 0.0578 - acc: 0.9819\n",
      "Epoch 32/100\n",
      "2376/2376 [==============================] - 1s 229us/step - loss: 0.0563 - acc: 0.9832\n",
      "Epoch 33/100\n",
      "2376/2376 [==============================] - 1s 218us/step - loss: 0.0554 - acc: 0.9840\n",
      "Epoch 34/100\n",
      "2376/2376 [==============================] - 1s 214us/step - loss: 0.0557 - acc: 0.9827\n",
      "Epoch 35/100\n",
      "2376/2376 [==============================] - 1s 234us/step - loss: 0.0551 - acc: 0.9832 0s - loss: 0.0378 - a\n",
      "Epoch 36/100\n",
      "2376/2376 [==============================] - 1s 224us/step - loss: 0.0545 - acc: 0.9848\n",
      "Epoch 37/100\n",
      "2376/2376 [==============================] - 1s 225us/step - loss: 0.0544 - acc: 0.9840\n",
      "Epoch 38/100\n",
      "2376/2376 [==============================] - 1s 221us/step - loss: 0.0539 - acc: 0.9836\n",
      "Epoch 39/100\n",
      "2376/2376 [==============================] - 1s 220us/step - loss: 0.0531 - acc: 0.9827\n",
      "Epoch 40/100\n",
      "2376/2376 [==============================] - 1s 233us/step - loss: 0.0528 - acc: 0.9823\n",
      "Epoch 41/100\n",
      "2376/2376 [==============================] - 1s 234us/step - loss: 0.0531 - acc: 0.9832\n",
      "Epoch 42/100\n",
      "2376/2376 [==============================] - 1s 230us/step - loss: 0.0517 - acc: 0.9844\n",
      "Epoch 43/100\n",
      "2376/2376 [==============================] - 1s 213us/step - loss: 0.0518 - acc: 0.9832\n",
      "Epoch 44/100\n",
      "2376/2376 [==============================] - 1s 219us/step - loss: 0.0509 - acc: 0.9823\n",
      "Epoch 45/100\n",
      "2376/2376 [==============================] - 1s 216us/step - loss: 0.0512 - acc: 0.9832\n",
      "Epoch 46/100\n",
      "2376/2376 [==============================] - 1s 218us/step - loss: 0.0501 - acc: 0.9823\n",
      "Epoch 47/100\n",
      "2376/2376 [==============================] - 1s 218us/step - loss: 0.0504 - acc: 0.9840\n",
      "Epoch 48/100\n",
      "2376/2376 [==============================] - 1s 266us/step - loss: 0.0496 - acc: 0.9848\n",
      "Epoch 49/100\n",
      "2376/2376 [==============================] - 1s 416us/step - loss: 0.0506 - acc: 0.9827\n",
      "Epoch 50/100\n",
      "2376/2376 [==============================] - 1s 445us/step - loss: 0.0486 - acc: 0.9853\n",
      "Epoch 51/100\n",
      "2376/2376 [==============================] - 1s 306us/step - loss: 0.0494 - acc: 0.9848\n",
      "Epoch 52/100\n",
      "2376/2376 [==============================] - 1s 264us/step - loss: 0.0486 - acc: 0.9848\n",
      "Epoch 53/100\n",
      "2376/2376 [==============================] - 1s 292us/step - loss: 0.0482 - acc: 0.9861\n",
      "Epoch 54/100\n",
      "2376/2376 [==============================] - 1s 254us/step - loss: 0.0489 - acc: 0.9848\n",
      "Epoch 55/100\n",
      "2376/2376 [==============================] - 1s 235us/step - loss: 0.0470 - acc: 0.9853\n",
      "Epoch 56/100\n",
      "2376/2376 [==============================] - 1s 224us/step - loss: 0.0474 - acc: 0.9853\n",
      "Epoch 57/100\n",
      "2376/2376 [==============================] - 0s 209us/step - loss: 0.0479 - acc: 0.9848\n",
      "Epoch 58/100\n",
      "2376/2376 [==============================] - 1s 211us/step - loss: 0.0466 - acc: 0.9848\n",
      "Epoch 59/100\n",
      "2376/2376 [==============================] - 1s 314us/step - loss: 0.0470 - acc: 0.9857\n",
      "Epoch 60/100\n",
      "2376/2376 [==============================] - 1s 269us/step - loss: 0.0467 - acc: 0.9840\n",
      "Epoch 61/100\n",
      "2376/2376 [==============================] - 1s 459us/step - loss: 0.0463 - acc: 0.9857ETA: 2s - loss: 0.007\n",
      "Epoch 62/100\n",
      "2376/2376 [==============================] - 1s 310us/step - loss: 0.0458 - acc: 0.9853\n",
      "Epoch 63/100\n",
      "2376/2376 [==============================] - 1s 284us/step - loss: 0.0456 - acc: 0.9861\n",
      "Epoch 64/100\n",
      "2376/2376 [==============================] - 1s 312us/step - loss: 0.0446 - acc: 0.9870\n",
      "Epoch 65/100\n",
      "2376/2376 [==============================] - 1s 319us/step - loss: 0.0451 - acc: 0.9861\n",
      "Epoch 66/100\n",
      "2376/2376 [==============================] - 1s 274us/step - loss: 0.0439 - acc: 0.9870\n",
      "Epoch 67/100\n",
      "2376/2376 [==============================] - 1s 266us/step - loss: 0.0441 - acc: 0.9857\n",
      "Epoch 68/100\n",
      "2376/2376 [==============================] - 1s 321us/step - loss: 0.0427 - acc: 0.9857\n",
      "Epoch 69/100\n",
      "2376/2376 [==============================] - ETA: 0s - loss: 0.0434 - acc: 0.985 - 1s 283us/step - loss: 0.0429 - acc: 0.9861\n",
      "Epoch 70/100\n",
      "2376/2376 [==============================] - 1s 232us/step - loss: 0.0432 - acc: 0.9870\n",
      "Epoch 71/100\n",
      "2376/2376 [==============================] - 1s 438us/step - loss: 0.0418 - acc: 0.9865\n",
      "Epoch 72/100\n",
      "2376/2376 [==============================] - 1s 282us/step - loss: 0.0433 - acc: 0.9870\n",
      "Epoch 73/100\n",
      "2376/2376 [==============================] - 1s 408us/step - loss: 0.0422 - acc: 0.9878\n",
      "Epoch 74/100\n",
      "2376/2376 [==============================] - 1s 216us/step - loss: 0.0408 - acc: 0.9874\n",
      "Epoch 75/100\n",
      "2376/2376 [==============================] - 1s 284us/step - loss: 0.0422 - acc: 0.9865\n",
      "Epoch 76/100\n",
      "2376/2376 [==============================] - 1s 290us/step - loss: 0.0410 - acc: 0.9857\n",
      "Epoch 77/100\n",
      "2376/2376 [==============================] - 1s 448us/step - loss: 0.0406 - acc: 0.9874\n",
      "Epoch 78/100\n",
      "2376/2376 [==============================] - 1s 346us/step - loss: 0.0412 - acc: 0.9870\n",
      "Epoch 79/100\n",
      "2376/2376 [==============================] - 1s 288us/step - loss: 0.0411 - acc: 0.9861\n",
      "Epoch 80/100\n",
      "2376/2376 [==============================] - 1s 341us/step - loss: 0.0402 - acc: 0.9870\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2376/2376 [==============================] - 1s 282us/step - loss: 0.0406 - acc: 0.9870\n",
      "Epoch 82/100\n",
      "2376/2376 [==============================] - 1s 332us/step - loss: 0.0405 - acc: 0.9870\n",
      "Epoch 83/100\n",
      "2376/2376 [==============================] - 1s 225us/step - loss: 0.0399 - acc: 0.9874\n",
      "Epoch 84/100\n",
      "2376/2376 [==============================] - 0s 203us/step - loss: 0.0406 - acc: 0.9874\n",
      "Epoch 85/100\n",
      "2376/2376 [==============================] - 0s 193us/step - loss: 0.0401 - acc: 0.9861\n",
      "Epoch 86/100\n",
      "2376/2376 [==============================] - 0s 190us/step - loss: 0.0402 - acc: 0.9865\n",
      "Epoch 87/100\n",
      "2376/2376 [==============================] - 0s 205us/step - loss: 0.0382 - acc: 0.9882 0s - loss: 0.0463 - acc:\n",
      "Epoch 88/100\n",
      "2376/2376 [==============================] - 0s 194us/step - loss: 0.0380 - acc: 0.9870\n",
      "Epoch 89/100\n",
      "2376/2376 [==============================] - 0s 198us/step - loss: 0.0388 - acc: 0.9870\n",
      "Epoch 90/100\n",
      "2376/2376 [==============================] - 0s 196us/step - loss: 0.0377 - acc: 0.9882\n",
      "Epoch 91/100\n",
      "2376/2376 [==============================] - 0s 209us/step - loss: 0.0383 - acc: 0.9886\n",
      "Epoch 92/100\n",
      "2376/2376 [==============================] - 1s 299us/step - loss: 0.0375 - acc: 0.9882\n",
      "Epoch 93/100\n",
      "2376/2376 [==============================] - 1s 246us/step - loss: 0.0376 - acc: 0.9886\n",
      "Epoch 94/100\n",
      "2376/2376 [==============================] - 1s 243us/step - loss: 0.0383 - acc: 0.9886\n",
      "Epoch 95/100\n",
      "2376/2376 [==============================] - 1s 222us/step - loss: 0.0376 - acc: 0.9886\n",
      "Epoch 96/100\n",
      "2376/2376 [==============================] - 1s 219us/step - loss: 0.0376 - acc: 0.9886\n",
      "Epoch 97/100\n",
      "2376/2376 [==============================] - 0s 203us/step - loss: 0.0355 - acc: 0.9895\n",
      "Epoch 98/100\n",
      "2376/2376 [==============================] - 1s 211us/step - loss: 0.0366 - acc: 0.9870\n",
      "Epoch 99/100\n",
      "2376/2376 [==============================] - 0s 207us/step - loss: 0.0367 - acc: 0.9891\n",
      "Epoch 100/100\n",
      "2376/2376 [==============================] - 1s 225us/step - loss: 0.0371 - acc: 0.9870\n",
      "0.976010101010101\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a17165128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2376/2376 [==============================] - 1s 387us/step - loss: 0.5736 - acc: 0.8443\n",
      "Epoch 2/100\n",
      "2376/2376 [==============================] - 0s 200us/step - loss: 0.1830 - acc: 0.9558\n",
      "Epoch 3/100\n",
      "2376/2376 [==============================] - 0s 208us/step - loss: 0.1222 - acc: 0.9625\n",
      "Epoch 4/100\n",
      "2376/2376 [==============================] - 0s 203us/step - loss: 0.1140 - acc: 0.9642\n",
      "Epoch 5/100\n",
      "2376/2376 [==============================] - 0s 203us/step - loss: 0.1115 - acc: 0.9646\n",
      "Epoch 6/100\n",
      "2376/2376 [==============================] - 0s 204us/step - loss: 0.1100 - acc: 0.9646\n",
      "Epoch 7/100\n",
      "2376/2376 [==============================] - 0s 205us/step - loss: 0.1084 - acc: 0.9651\n",
      "Epoch 8/100\n",
      "2376/2376 [==============================] - 0s 202us/step - loss: 0.1072 - acc: 0.9655\n",
      "Epoch 9/100\n",
      "2376/2376 [==============================] - 1s 212us/step - loss: 0.1054 - acc: 0.9651\n",
      "Epoch 10/100\n",
      "2376/2376 [==============================] - 0s 202us/step - loss: 0.1044 - acc: 0.9663\n",
      "Epoch 11/100\n",
      "2376/2376 [==============================] - 1s 215us/step - loss: 0.1030 - acc: 0.9651\n",
      "Epoch 12/100\n",
      "2376/2376 [==============================] - 1s 216us/step - loss: 0.1016 - acc: 0.9663\n",
      "Epoch 13/100\n",
      "2376/2376 [==============================] - 0s 207us/step - loss: 0.1009 - acc: 0.9655\n",
      "Epoch 14/100\n",
      "2376/2376 [==============================] - 1s 223us/step - loss: 0.0996 - acc: 0.9651\n",
      "Epoch 15/100\n",
      "2376/2376 [==============================] - 0s 209us/step - loss: 0.0990 - acc: 0.9659\n",
      "Epoch 16/100\n",
      "2376/2376 [==============================] - 1s 211us/step - loss: 0.0980 - acc: 0.9668\n",
      "Epoch 17/100\n",
      "2376/2376 [==============================] - 1s 211us/step - loss: 0.0978 - acc: 0.9655\n",
      "Epoch 18/100\n",
      "2376/2376 [==============================] - 1s 211us/step - loss: 0.0973 - acc: 0.9668\n",
      "Epoch 19/100\n",
      "2376/2376 [==============================] - 0s 208us/step - loss: 0.0969 - acc: 0.9655\n",
      "Epoch 20/100\n",
      "2376/2376 [==============================] - 1s 221us/step - loss: 0.0965 - acc: 0.9659\n",
      "Epoch 21/100\n",
      "2376/2376 [==============================] - 1s 267us/step - loss: 0.0963 - acc: 0.9651\n",
      "Epoch 22/100\n",
      "2376/2376 [==============================] - 1s 233us/step - loss: 0.0958 - acc: 0.9659\n",
      "Epoch 23/100\n",
      "2376/2376 [==============================] - 1s 213us/step - loss: 0.0958 - acc: 0.9663\n",
      "Epoch 24/100\n",
      "2376/2376 [==============================] - 0s 207us/step - loss: 0.0957 - acc: 0.9663\n",
      "Epoch 25/100\n",
      "2376/2376 [==============================] - 1s 215us/step - loss: 0.0952 - acc: 0.9655\n",
      "Epoch 26/100\n",
      "2376/2376 [==============================] - 1s 272us/step - loss: 0.0950 - acc: 0.9651\n",
      "Epoch 27/100\n",
      "2376/2376 [==============================] - 1s 298us/step - loss: 0.0951 - acc: 0.9659\n",
      "Epoch 28/100\n",
      "2376/2376 [==============================] - 1s 243us/step - loss: 0.0947 - acc: 0.9659\n",
      "Epoch 29/100\n",
      "2376/2376 [==============================] - 1s 212us/step - loss: 0.0946 - acc: 0.9655\n",
      "Epoch 30/100\n",
      "2376/2376 [==============================] - 1s 229us/step - loss: 0.0944 - acc: 0.9655\n",
      "Epoch 31/100\n",
      "2376/2376 [==============================] - 1s 214us/step - loss: 0.0940 - acc: 0.9663\n",
      "Epoch 32/100\n",
      "2376/2376 [==============================] - 1s 227us/step - loss: 0.0943 - acc: 0.9651 0s - loss: 0.1015 - ac\n",
      "Epoch 33/100\n",
      "2376/2376 [==============================] - 1s 228us/step - loss: 0.0943 - acc: 0.9663\n",
      "Epoch 34/100\n",
      "2376/2376 [==============================] - 1s 220us/step - loss: 0.0940 - acc: 0.9655\n",
      "Epoch 35/100\n",
      "2376/2376 [==============================] - 1s 231us/step - loss: 0.0937 - acc: 0.9659\n",
      "Epoch 36/100\n",
      "2376/2376 [==============================] - 1s 222us/step - loss: 0.0935 - acc: 0.9659\n",
      "Epoch 37/100\n",
      "2376/2376 [==============================] - 1s 250us/step - loss: 0.0935 - acc: 0.9663\n",
      "Epoch 38/100\n",
      "2376/2376 [==============================] - 1s 263us/step - loss: 0.0934 - acc: 0.9659\n",
      "Epoch 39/100\n",
      "2376/2376 [==============================] - 1s 306us/step - loss: 0.0936 - acc: 0.9668\n",
      "Epoch 40/100\n",
      "2376/2376 [==============================] - 1s 278us/step - loss: 0.0934 - acc: 0.9655\n",
      "Epoch 41/100\n",
      "2376/2376 [==============================] - 1s 227us/step - loss: 0.0935 - acc: 0.9668\n",
      "Epoch 42/100\n",
      "2376/2376 [==============================] - 1s 219us/step - loss: 0.0939 - acc: 0.9655\n",
      "Epoch 43/100\n",
      "2376/2376 [==============================] - 1s 222us/step - loss: 0.0931 - acc: 0.9659\n",
      "Epoch 44/100\n",
      "2376/2376 [==============================] - 1s 212us/step - loss: 0.0930 - acc: 0.9659\n",
      "Epoch 45/100\n",
      "2376/2376 [==============================] - 1s 228us/step - loss: 0.0929 - acc: 0.9659\n",
      "Epoch 46/100\n",
      "2376/2376 [==============================] - 1s 224us/step - loss: 0.0929 - acc: 0.9651\n",
      "Epoch 47/100\n",
      "2376/2376 [==============================] - 1s 215us/step - loss: 0.0929 - acc: 0.9659\n",
      "Epoch 48/100\n",
      "2376/2376 [==============================] - 1s 230us/step - loss: 0.0928 - acc: 0.9676\n",
      "Epoch 49/100\n",
      "2376/2376 [==============================] - 1s 252us/step - loss: 0.0926 - acc: 0.9651\n",
      "Epoch 50/100\n",
      "2376/2376 [==============================] - 1s 245us/step - loss: 0.0927 - acc: 0.9668\n",
      "Epoch 51/100\n",
      "2376/2376 [==============================] - 1s 243us/step - loss: 0.0926 - acc: 0.9663\n",
      "Epoch 52/100\n",
      "2376/2376 [==============================] - 1s 231us/step - loss: 0.0926 - acc: 0.9668\n",
      "Epoch 53/100\n",
      "2376/2376 [==============================] - 1s 225us/step - loss: 0.0922 - acc: 0.9663\n",
      "Epoch 54/100\n",
      "2376/2376 [==============================] - 1s 218us/step - loss: 0.0927 - acc: 0.9659\n",
      "Epoch 55/100\n",
      "2376/2376 [==============================] - 1s 254us/step - loss: 0.0923 - acc: 0.9659\n",
      "Epoch 56/100\n",
      "2376/2376 [==============================] - 1s 251us/step - loss: 0.0924 - acc: 0.9663\n",
      "Epoch 57/100\n",
      "2376/2376 [==============================] - 1s 229us/step - loss: 0.0923 - acc: 0.9672\n",
      "Epoch 58/100\n",
      "2376/2376 [==============================] - 1s 228us/step - loss: 0.0924 - acc: 0.9672\n",
      "Epoch 59/100\n",
      "2376/2376 [==============================] - 1s 232us/step - loss: 0.0925 - acc: 0.9668\n",
      "Epoch 60/100\n",
      "2376/2376 [==============================] - 1s 228us/step - loss: 0.0923 - acc: 0.9663\n",
      "Epoch 61/100\n",
      "2376/2376 [==============================] - 1s 217us/step - loss: 0.0922 - acc: 0.9672\n",
      "Epoch 62/100\n",
      "2376/2376 [==============================] - 1s 244us/step - loss: 0.0924 - acc: 0.9672\n",
      "Epoch 63/100\n",
      "2376/2376 [==============================] - 1s 230us/step - loss: 0.0923 - acc: 0.9676\n",
      "Epoch 64/100\n",
      "2376/2376 [==============================] - 1s 235us/step - loss: 0.0921 - acc: 0.9680\n",
      "Epoch 65/100\n",
      "2376/2376 [==============================] - 1s 242us/step - loss: 0.0920 - acc: 0.9676\n",
      "Epoch 66/100\n",
      "2376/2376 [==============================] - 1s 218us/step - loss: 0.0917 - acc: 0.9676\n",
      "Epoch 67/100\n",
      "2376/2376 [==============================] - 1s 222us/step - loss: 0.0917 - acc: 0.9672\n",
      "Epoch 68/100\n",
      "2376/2376 [==============================] - 1s 227us/step - loss: 0.0920 - acc: 0.9676\n",
      "Epoch 69/100\n",
      "2376/2376 [==============================] - 1s 225us/step - loss: 0.0922 - acc: 0.9668\n",
      "Epoch 70/100\n",
      "2376/2376 [==============================] - 1s 221us/step - loss: 0.0919 - acc: 0.9676\n",
      "Epoch 71/100\n",
      "2376/2376 [==============================] - 1s 240us/step - loss: 0.0917 - acc: 0.9680\n",
      "Epoch 72/100\n",
      "2376/2376 [==============================] - 1s 219us/step - loss: 0.0914 - acc: 0.9684\n",
      "Epoch 73/100\n",
      "2376/2376 [==============================] - 1s 223us/step - loss: 0.0918 - acc: 0.9659\n",
      "Epoch 74/100\n",
      "2376/2376 [==============================] - 0s 208us/step - loss: 0.0913 - acc: 0.9668\n",
      "Epoch 75/100\n",
      "2376/2376 [==============================] - 1s 216us/step - loss: 0.0919 - acc: 0.9676\n",
      "Epoch 76/100\n",
      "2376/2376 [==============================] - 1s 222us/step - loss: 0.0911 - acc: 0.9668\n",
      "Epoch 77/100\n",
      "2376/2376 [==============================] - 1s 211us/step - loss: 0.0921 - acc: 0.9676\n",
      "Epoch 78/100\n",
      "2376/2376 [==============================] - 1s 213us/step - loss: 0.0909 - acc: 0.9676\n",
      "Epoch 79/100\n",
      "2376/2376 [==============================] - 1s 257us/step - loss: 0.0915 - acc: 0.9676\n",
      "Epoch 80/100\n",
      "2376/2376 [==============================] - 1s 278us/step - loss: 0.0916 - acc: 0.9668\n",
      "Epoch 81/100\n",
      "2376/2376 [==============================] - 1s 270us/step - loss: 0.0911 - acc: 0.9668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      "2376/2376 [==============================] - 1s 241us/step - loss: 0.0910 - acc: 0.9676\n",
      "Epoch 83/100\n",
      "2376/2376 [==============================] - 1s 249us/step - loss: 0.0912 - acc: 0.9672\n",
      "Epoch 84/100\n",
      "2376/2376 [==============================] - 0s 203us/step - loss: 0.0912 - acc: 0.9668\n",
      "Epoch 85/100\n",
      "2376/2376 [==============================] - 0s 206us/step - loss: 0.0905 - acc: 0.9684\n",
      "Epoch 86/100\n",
      "2376/2376 [==============================] - 0s 204us/step - loss: 0.0920 - acc: 0.9680\n",
      "Epoch 87/100\n",
      "2376/2376 [==============================] - 0s 193us/step - loss: 0.0909 - acc: 0.9668\n",
      "Epoch 88/100\n",
      "2376/2376 [==============================] - 0s 204us/step - loss: 0.0907 - acc: 0.9672\n",
      "Epoch 89/100\n",
      "2376/2376 [==============================] - 0s 203us/step - loss: 0.0915 - acc: 0.9663\n",
      "Epoch 90/100\n",
      "2376/2376 [==============================] - 0s 210us/step - loss: 0.0911 - acc: 0.9668\n",
      "Epoch 91/100\n",
      "2376/2376 [==============================] - 0s 201us/step - loss: 0.0905 - acc: 0.9659\n",
      "Epoch 92/100\n",
      "2376/2376 [==============================] - 0s 208us/step - loss: 0.0908 - acc: 0.9668\n",
      "Epoch 93/100\n",
      "2376/2376 [==============================] - 0s 206us/step - loss: 0.0913 - acc: 0.9680\n",
      "Epoch 94/100\n",
      "2376/2376 [==============================] - 0s 201us/step - loss: 0.0900 - acc: 0.9668\n",
      "Epoch 95/100\n",
      "2376/2376 [==============================] - 0s 201us/step - loss: 0.0911 - acc: 0.9663\n",
      "Epoch 96/100\n",
      "2376/2376 [==============================] - 0s 202us/step - loss: 0.0905 - acc: 0.9676\n",
      "Epoch 97/100\n",
      "2376/2376 [==============================] - 0s 203us/step - loss: 0.0907 - acc: 0.9663\n",
      "Epoch 98/100\n",
      "2376/2376 [==============================] - 0s 208us/step - loss: 0.0903 - acc: 0.9668\n",
      "Epoch 99/100\n",
      "2376/2376 [==============================] - 0s 194us/step - loss: 0.0901 - acc: 0.9676\n",
      "Epoch 100/100\n",
      "2376/2376 [==============================] - 0s 198us/step - loss: 0.0909 - acc: 0.9672\n",
      "0.9722222222222222\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsDAzs5r+Fz9gYrmlymNJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a176db668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2376/2376 [==============================] - 1s 427us/step - loss: 0.5874 - acc: 0.7925\n",
      "Epoch 2/100\n",
      "2376/2376 [==============================] - 1s 243us/step - loss: 0.1969 - acc: 0.9251\n",
      "Epoch 3/100\n",
      "2376/2376 [==============================] - 1s 211us/step - loss: 0.1042 - acc: 0.9651\n",
      "Epoch 4/100\n",
      "2376/2376 [==============================] - 1s 212us/step - loss: 0.0884 - acc: 0.9714\n",
      "Epoch 5/100\n",
      "2376/2376 [==============================] - 0s 207us/step - loss: 0.0839 - acc: 0.9764\n",
      "Epoch 6/100\n",
      "2376/2376 [==============================] - 1s 246us/step - loss: 0.0812 - acc: 0.9777\n",
      "Epoch 7/100\n",
      "2376/2376 [==============================] - 1s 253us/step - loss: 0.0796 - acc: 0.9777\n",
      "Epoch 8/100\n",
      "2376/2376 [==============================] - 1s 234us/step - loss: 0.0776 - acc: 0.9790\n",
      "Epoch 9/100\n",
      "2376/2376 [==============================] - 0s 205us/step - loss: 0.0747 - acc: 0.9781\n",
      "Epoch 10/100\n",
      "2376/2376 [==============================] - 1s 211us/step - loss: 0.0733 - acc: 0.9790\n",
      "Epoch 11/100\n",
      "2376/2376 [==============================] - 0s 204us/step - loss: 0.0722 - acc: 0.9794\n",
      "Epoch 12/100\n",
      "2376/2376 [==============================] - 1s 211us/step - loss: 0.0711 - acc: 0.9790\n",
      "Epoch 13/100\n",
      "2376/2376 [==============================] - 1s 227us/step - loss: 0.0700 - acc: 0.9781\n",
      "Epoch 14/100\n",
      "2376/2376 [==============================] - 1s 223us/step - loss: 0.0683 - acc: 0.9785\n",
      "Epoch 15/100\n",
      "2376/2376 [==============================] - 0s 210us/step - loss: 0.0673 - acc: 0.9785\n",
      "Epoch 16/100\n",
      "2376/2376 [==============================] - 1s 216us/step - loss: 0.0664 - acc: 0.9790\n",
      "Epoch 17/100\n",
      "2376/2376 [==============================] - 1s 222us/step - loss: 0.0654 - acc: 0.9790\n",
      "Epoch 18/100\n",
      "2376/2376 [==============================] - 1s 225us/step - loss: 0.0644 - acc: 0.9806\n",
      "Epoch 19/100\n",
      "2376/2376 [==============================] - 1s 214us/step - loss: 0.0639 - acc: 0.9806\n",
      "Epoch 20/100\n",
      "2376/2376 [==============================] - 1s 214us/step - loss: 0.0637 - acc: 0.9802\n",
      "Epoch 21/100\n",
      "2376/2376 [==============================] - 1s 228us/step - loss: 0.0625 - acc: 0.9823\n",
      "Epoch 22/100\n",
      "2376/2376 [==============================] - 1s 228us/step - loss: 0.0622 - acc: 0.9798\n",
      "Epoch 23/100\n",
      "2376/2376 [==============================] - 0s 210us/step - loss: 0.0613 - acc: 0.9806\n",
      "Epoch 24/100\n",
      "2376/2376 [==============================] - 0s 207us/step - loss: 0.0609 - acc: 0.9819\n",
      "Epoch 25/100\n",
      "2376/2376 [==============================] - 0s 207us/step - loss: 0.0606 - acc: 0.9815\n",
      "Epoch 26/100\n",
      "2376/2376 [==============================] - 1s 212us/step - loss: 0.0599 - acc: 0.9819\n",
      "Epoch 27/100\n",
      "2376/2376 [==============================] - 1s 214us/step - loss: 0.0598 - acc: 0.9806\n",
      "Epoch 28/100\n",
      "2376/2376 [==============================] - 1s 212us/step - loss: 0.0589 - acc: 0.9823\n",
      "Epoch 29/100\n",
      "2376/2376 [==============================] - 0s 208us/step - loss: 0.0589 - acc: 0.9823\n",
      "Epoch 30/100\n",
      "2376/2376 [==============================] - 0s 210us/step - loss: 0.0587 - acc: 0.9815\n",
      "Epoch 31/100\n",
      "2376/2376 [==============================] - 0s 210us/step - loss: 0.0586 - acc: 0.9819\n",
      "Epoch 32/100\n",
      "2376/2376 [==============================] - 0s 206us/step - loss: 0.0577 - acc: 0.9827\n",
      "Epoch 33/100\n",
      "2376/2376 [==============================] - 0s 208us/step - loss: 0.0580 - acc: 0.9811\n",
      "Epoch 34/100\n",
      "2376/2376 [==============================] - 0s 207us/step - loss: 0.0573 - acc: 0.9823\n",
      "Epoch 35/100\n",
      "2376/2376 [==============================] - 1s 211us/step - loss: 0.0572 - acc: 0.9823\n",
      "Epoch 36/100\n",
      "2376/2376 [==============================] - 1s 246us/step - loss: 0.0571 - acc: 0.9823\n",
      "Epoch 37/100\n",
      "2376/2376 [==============================] - 1s 279us/step - loss: 0.0569 - acc: 0.9823\n",
      "Epoch 38/100\n",
      "2376/2376 [==============================] - 1s 275us/step - loss: 0.0565 - acc: 0.9819 0s - loss: 0.0542 - \n",
      "Epoch 39/100\n",
      "2376/2376 [==============================] - 1s 261us/step - loss: 0.0561 - acc: 0.9823 0s - loss: 0.0403 - \n",
      "Epoch 40/100\n",
      "2376/2376 [==============================] - 1s 220us/step - loss: 0.0557 - acc: 0.9836\n",
      "Epoch 41/100\n",
      "2376/2376 [==============================] - 1s 216us/step - loss: 0.0556 - acc: 0.9827\n",
      "Epoch 42/100\n",
      "2376/2376 [==============================] - 1s 216us/step - loss: 0.0552 - acc: 0.9815\n",
      "Epoch 43/100\n",
      "2376/2376 [==============================] - 1s 216us/step - loss: 0.0548 - acc: 0.9832\n",
      "Epoch 44/100\n",
      "2376/2376 [==============================] - 1s 215us/step - loss: 0.0547 - acc: 0.9836\n",
      "Epoch 45/100\n",
      "2376/2376 [==============================] - 1s 223us/step - loss: 0.0544 - acc: 0.9827\n",
      "Epoch 46/100\n",
      "2376/2376 [==============================] - 1s 216us/step - loss: 0.0540 - acc: 0.9827\n",
      "Epoch 47/100\n",
      "2376/2376 [==============================] - 1s 221us/step - loss: 0.0541 - acc: 0.9819\n",
      "Epoch 48/100\n",
      "2376/2376 [==============================] - 1s 229us/step - loss: 0.0540 - acc: 0.9840\n",
      "Epoch 49/100\n",
      "2376/2376 [==============================] - 1s 227us/step - loss: 0.0538 - acc: 0.9832\n",
      "Epoch 50/100\n",
      "2376/2376 [==============================] - 1s 223us/step - loss: 0.0536 - acc: 0.9840\n",
      "Epoch 51/100\n",
      "2376/2376 [==============================] - 1s 216us/step - loss: 0.0530 - acc: 0.9836\n",
      "Epoch 52/100\n",
      "2376/2376 [==============================] - 1s 214us/step - loss: 0.0531 - acc: 0.9836\n",
      "Epoch 53/100\n",
      "2376/2376 [==============================] - 1s 264us/step - loss: 0.0529 - acc: 0.9836\n",
      "Epoch 54/100\n",
      "2376/2376 [==============================] - 1s 250us/step - loss: 0.0529 - acc: 0.9823\n",
      "Epoch 55/100\n",
      "2376/2376 [==============================] - 1s 352us/step - loss: 0.0523 - acc: 0.9853 1s - loss: 0.0563 - ac\n",
      "Epoch 56/100\n",
      "2376/2376 [==============================] - 1s 233us/step - loss: 0.0524 - acc: 0.9844\n",
      "Epoch 57/100\n",
      "2376/2376 [==============================] - 1s 255us/step - loss: 0.0520 - acc: 0.9840\n",
      "Epoch 58/100\n",
      "2376/2376 [==============================] - 1s 301us/step - loss: 0.0516 - acc: 0.9840\n",
      "Epoch 59/100\n",
      "2376/2376 [==============================] - 1s 274us/step - loss: 0.0514 - acc: 0.9844\n",
      "Epoch 60/100\n",
      "2376/2376 [==============================] - 1s 271us/step - loss: 0.0514 - acc: 0.9844\n",
      "Epoch 61/100\n",
      "2376/2376 [==============================] - 1s 242us/step - loss: 0.0509 - acc: 0.9844\n",
      "Epoch 62/100\n",
      "2376/2376 [==============================] - 1s 233us/step - loss: 0.0504 - acc: 0.9840\n",
      "Epoch 63/100\n",
      "2376/2376 [==============================] - 1s 227us/step - loss: 0.0511 - acc: 0.9844\n",
      "Epoch 64/100\n",
      "2376/2376 [==============================] - 1s 276us/step - loss: 0.0501 - acc: 0.9848\n",
      "Epoch 65/100\n",
      "2376/2376 [==============================] - 1s 284us/step - loss: 0.0503 - acc: 0.9844\n",
      "Epoch 66/100\n",
      "2376/2376 [==============================] - 1s 270us/step - loss: 0.0500 - acc: 0.9853\n",
      "Epoch 67/100\n",
      "2376/2376 [==============================] - 1s 224us/step - loss: 0.0494 - acc: 0.9844\n",
      "Epoch 68/100\n",
      "2376/2376 [==============================] - 1s 211us/step - loss: 0.0494 - acc: 0.9848\n",
      "Epoch 69/100\n",
      "2376/2376 [==============================] - 1s 229us/step - loss: 0.0493 - acc: 0.9853\n",
      "Epoch 70/100\n",
      "2376/2376 [==============================] - 1s 228us/step - loss: 0.0491 - acc: 0.9844\n",
      "Epoch 71/100\n",
      "2376/2376 [==============================] - 1s 213us/step - loss: 0.0486 - acc: 0.9853\n",
      "Epoch 72/100\n",
      "2376/2376 [==============================] - 1s 216us/step - loss: 0.0486 - acc: 0.9840\n",
      "Epoch 73/100\n",
      "2376/2376 [==============================] - 1s 212us/step - loss: 0.0485 - acc: 0.9848\n",
      "Epoch 74/100\n",
      "2376/2376 [==============================] - 1s 212us/step - loss: 0.0480 - acc: 0.9848\n",
      "Epoch 75/100\n",
      "2376/2376 [==============================] - 1s 216us/step - loss: 0.0481 - acc: 0.9848\n",
      "Epoch 76/100\n",
      "2376/2376 [==============================] - 1s 235us/step - loss: 0.0478 - acc: 0.9844\n",
      "Epoch 77/100\n",
      "2376/2376 [==============================] - 1s 216us/step - loss: 0.0475 - acc: 0.9844\n",
      "Epoch 78/100\n",
      "2376/2376 [==============================] - 1s 226us/step - loss: 0.0471 - acc: 0.9857\n",
      "Epoch 79/100\n",
      "2376/2376 [==============================] - 1s 213us/step - loss: 0.0470 - acc: 0.9844\n",
      "Epoch 80/100\n",
      "2376/2376 [==============================] - 1s 212us/step - loss: 0.0471 - acc: 0.9848\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2376/2376 [==============================] - 0s 209us/step - loss: 0.0467 - acc: 0.9853\n",
      "Epoch 82/100\n",
      "2376/2376 [==============================] - 1s 211us/step - loss: 0.0473 - acc: 0.9844\n",
      "Epoch 83/100\n",
      "2376/2376 [==============================] - 1s 213us/step - loss: 0.0462 - acc: 0.9857\n",
      "Epoch 84/100\n",
      "2376/2376 [==============================] - 1s 213us/step - loss: 0.0462 - acc: 0.9865\n",
      "Epoch 85/100\n",
      "2376/2376 [==============================] - 1s 243us/step - loss: 0.0461 - acc: 0.9853\n",
      "Epoch 86/100\n",
      "2376/2376 [==============================] - 1s 211us/step - loss: 0.0459 - acc: 0.9857\n",
      "Epoch 87/100\n",
      "2376/2376 [==============================] - 1s 220us/step - loss: 0.0457 - acc: 0.9857\n",
      "Epoch 88/100\n",
      "2376/2376 [==============================] - 1s 222us/step - loss: 0.0453 - acc: 0.9857\n",
      "Epoch 89/100\n",
      "2376/2376 [==============================] - 1s 218us/step - loss: 0.0457 - acc: 0.9861\n",
      "Epoch 90/100\n",
      "2376/2376 [==============================] - 1s 234us/step - loss: 0.0451 - acc: 0.9865\n",
      "Epoch 91/100\n",
      "2376/2376 [==============================] - 1s 227us/step - loss: 0.0453 - acc: 0.9865\n",
      "Epoch 92/100\n",
      "2376/2376 [==============================] - 1s 214us/step - loss: 0.0453 - acc: 0.9861\n",
      "Epoch 93/100\n",
      "2376/2376 [==============================] - 1s 225us/step - loss: 0.0446 - acc: 0.9861\n",
      "Epoch 94/100\n",
      "2376/2376 [==============================] - 1s 274us/step - loss: 0.0451 - acc: 0.9857\n",
      "Epoch 95/100\n",
      "2376/2376 [==============================] - 1s 276us/step - loss: 0.0448 - acc: 0.9870\n",
      "Epoch 96/100\n",
      "2376/2376 [==============================] - 1s 267us/step - loss: 0.0450 - acc: 0.9861\n",
      "Epoch 97/100\n",
      "2376/2376 [==============================] - 1s 227us/step - loss: 0.0445 - acc: 0.9861\n",
      "Epoch 98/100\n",
      "2376/2376 [==============================] - 1s 264us/step - loss: 0.0443 - acc: 0.9870\n",
      "Epoch 99/100\n",
      "2376/2376 [==============================] - 1s 253us/step - loss: 0.0443 - acc: 0.9865\n",
      "Epoch 100/100\n",
      "2376/2376 [==============================] - 1s 221us/step - loss: 0.0440 - acc: 0.9870\n",
      "0.9734848484848485\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nq4mDxMzMauIgMTOzmvx/BX/GeKQ3QzAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a17925f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import the keras lib and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import sklearn.metrics as metrics\n",
    "from keras.callbacks import History \n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def run_ANN(train_x,test_x,train_y,test_y):\n",
    "    #Initializing ANN\n",
    "    clf = Sequential()\n",
    "    hist = History()\n",
    "    clf.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = len(train_x[1])))\n",
    "    # Adding the second hidden layer\n",
    "    clf.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "    # Adding the output layer\n",
    "    clf.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "    clf.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    clf.fit(train_x, train_y, batch_size = 10, nb_epoch = 100 ,callbacks = [hist])\n",
    "    pred_y = clf.predict(test_x)\n",
    "    pred_y = (pred_y > 0.5)\n",
    "    \n",
    "        \n",
    "    print(metrics.accuracy_score(pred_y,test_y))\n",
    "\n",
    "    plt.plot(hist.history['acc'], color = 'red')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "        \n",
    "run_ANN(train_x,test_x,train_y,test_y)\n",
    "run_ANN(train_x_two_features, test_x_two_features, train_y_two_features, test_y_two_features)\n",
    "run_ANN(pca13_train_x, pca13_test_x, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Data preparation\n",
    "# path = '/Users/Kassi/Desktop/Gender_Recognition_by_Voice/voice.csv'\n",
    "# voice_data = pd.read_csv(path)\n",
    "# voice_data = voice_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # There are multiple steps we will take in this section to transform the original data into format which we can easily plug inside tensorflow's tensors.\n",
    "# # Creating 1-hot vector from the original labels\n",
    "# # Randomly shuffle data\n",
    "# # Create train/test datasets\n",
    "\n",
    "# voices = voice_data[:, :-1] \n",
    "# labels = voice_data[:, -1:]\n",
    "\n",
    "# # Classification: label(gender) = {male, female}  [1.0, 0.0], [0.0, 1.0]  \n",
    "# labels_tmp = []  \n",
    "# for label in labels:  \n",
    "#     tmp = []  \n",
    "#     if label[0] == 'male':  \n",
    "#         tmp = [1.0, 0.0]  \n",
    "#     else:   \n",
    "#         tmp = [0.0, 1.0]  \n",
    "#     labels_tmp.append(tmp)  \n",
    "# labels = np.array(labels_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # randomly shuffle data\n",
    "# voices_tmp = []  \n",
    "# lables_tmp = []  \n",
    "# index_shuf = range(len(voices)) \n",
    "# random.shuffle(index_shuf) \n",
    "# for i in index_shuf:  \n",
    "#     voices_tmp.append(voices[i])  \n",
    "#     lables_tmp.append(labels[i])  \n",
    "# voices = np.array(voices_tmp)  \n",
    "# labels = np.array(lables_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_x, test_x, train_y, test_y = train_test_split(voices, labels, test_size=0.2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we are all set to get started with creating the tensorflow graph. \n",
    "# It is important to understand that after this step there is no actual computation being done by tensorflow. \n",
    "# It just creates a lazy graph according to the nodes we create in the neural_network method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Learning Parameters\n",
    "# rate   =   0.0010  # training rate\n",
    "# epochs =  2000     # number of full training cycles (I select this number based on the trend of cost)\n",
    "# banch_size  = 68   # number of data points to train per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Network Parameters\n",
    "# n_hidden_1 = 512  # number of nodes in hidden layer 1\n",
    "# n_hidden_2 = 512  # number of nodes in hidden layer 2\n",
    "# n_input    = voices.shape[-1]  # 20\n",
    "# n_classes  = 2\n",
    "# n_samples  = len(voices)  # 460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X = tf.placeholder('float', [None,n_input])\n",
    "# Y = tf.placeholder('float', [None,n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# weights = {\n",
    "#     'w1' : tf.Variable(tf.random_normal([n_input,    n_hidden_1])),\n",
    "#     'w2' : tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "#     'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes ]))\n",
    "# }\n",
    "\n",
    "# biases = {\n",
    "#     'b1' : tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "#     'b2' : tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "#     'out': tf.Variable(tf.random_normal([n_classes ]))\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def neural_network(X, weights, biases):  \n",
    "#     # Hidden Layer 1\n",
    "#     layer1 = tf.matmul(X, weights['w1'])\n",
    "#     layer1 = tf.add(layer1, biases['b1'])\n",
    "#     layer1 = tf.nn.softmax(layer1)\n",
    "    \n",
    "#     # Hidden Layer 2\n",
    "#     layer2 = tf.matmul(layer1, weights['w2'])\n",
    "#     layer2 = tf.add(layer2, biases['b2'])\n",
    "#     layer2 = tf.nn.softmax(layer2)\n",
    "    \n",
    "#     # Output Layer\n",
    "#     output = tf.matmul(layer2, weights['out'])\n",
    "#     output = tf.add(output, biases['out'])\n",
    "#     output = tf.nn.softmax(output)\n",
    "    \n",
    "#     return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # In this section we define the optimizer and loss functions. \n",
    "# # We also define our notion of correct and incorrect prediction and what we mean by accuracy.\n",
    "\n",
    "# def train_neural_network(train_x,train_y,test_x,test_y):\n",
    "#     model = neural_network(X, weights, biases)\n",
    "#     f_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "#     f_optimizer = tf.train.AdamOptimizer(learning_rate=rate).minimize(f_cost)\n",
    "#     with tf.Session() as s:\n",
    "#         s.run(tf.global_variables_initializer())\n",
    "#         for epoch in range(epochs):\n",
    "#             cost_avg = 0.\n",
    "#             batch_total = len(train_x) // banch_size\n",
    "#             for banch in range(batch_total):\n",
    "#                 voice_banch = train_x[banch*banch_size:(banch+1)*(banch_size)]  \n",
    "#                 label_banch = train_y[banch*banch_size:(banch+1)*(banch_size)]        \n",
    "#                 _, cost = s.run([f_optimizer,f_cost], feed_dict={X:voice_banch, Y: label_banch})        \n",
    "#                 cost_avg += cost / batch_total\n",
    "        \n",
    "#             print('Epoch {}: cost={:.4f}'.format(epoch+1, cost_avg))\n",
    "    \n",
    "#         # testing\n",
    "        \n",
    "#         # This gives us a list of booleans\n",
    "#         prediction = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))  \n",
    "#         # We cast to floating point numbers and then take the mean\n",
    "#         accuracy = tf.reduce_mean(tf.cast(prediction, dtype=tf.float32))  \n",
    "#         accuracy = s.run(accuracy, feed_dict={X: train_x, Y: train_y})  \n",
    "#         print('In-sample accuracy in Neural Network: %s'  % (accuracy)) \n",
    "        \n",
    "#         accuracy = tf.reduce_mean(tf.cast(prediction, dtype=tf.float32)) \n",
    "#         accuracy = s.run(accuracy, feed_dict={X: test_x, Y: test_y})  \n",
    "#         print('Out-of-sample accuracy in Neural Network: %s'  % (accuracy)) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Execute tf graph\n",
    "# # Now that the graph is set up, we can run it\n",
    "\n",
    "# print('Accuracy in Neural Network with all features:')\n",
    "# train_neural_network(train_x,train_y,test_x,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are wondering if there is a big difference in the results when used Keras deep learning library with a TensorFlow backend vs. our own step-by-step algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Importing Libraries for ANN\n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# #Initialisng the ANN\n",
    "# classifier = Sequential()\n",
    "\n",
    "# #Input Layer and First Hidden Layer\n",
    "# classifier.add(Dense(units = 512, activation = 'softmax', input_dim = train_x.shape[1]))\n",
    "\n",
    "# #Adding the Second hidden layer\n",
    "# classifier.add(Dense(units = 512, activation = 'softmax'))\n",
    "\n",
    "# # Adding the output layer\n",
    "# classifier.add(Dense(units = 2, activation = 'softmax'))\n",
    "\n",
    "# #Compiling the ANN\n",
    "# classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Fitting ANN to the training set\n",
    "# classifier.fit(train_x, train_y, batch_size = 68, epochs = 2000, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Predicting the Test set results\n",
    "# pred_train_y = classifier.predict(train_x)\n",
    "# pred_train_y = (pred_train_y > 0.5)\n",
    "# # pred_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels_tmp = []  \n",
    "# for label in pred_train_y:  \n",
    "#     tmp = []  \n",
    "#     if label[0] == True:  \n",
    "#         tmp = [1,0]  \n",
    "#     else:   \n",
    "#         tmp = [0,1]  \n",
    "#     labels_tmp.append(tmp)  \n",
    "# new_result = np.array(labels_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# print('In-sample accuracy in Neural Network using Keras package (with all features):%s' % (accuracy_score(train_y, new_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Predicting the Test set results\n",
    "# pred_test_y = classifier.predict(test_x)\n",
    "# pred_test_y = (pred_test_y > 0.5)\n",
    "# # pred_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels_tmp = []  \n",
    "# for label in pred_test_y:  \n",
    "#     tmp = []  \n",
    "#     if label[0] == True:  \n",
    "#         tmp = [1,0]  \n",
    "#     else:   \n",
    "#         tmp = [0,1]  \n",
    "#     labels_tmp.append(tmp)  \n",
    "# new_result = np.array(labels_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('Out-of-sample accuracy in Neural Network using Keras package (with all features):%s' % (accuracy_score(test_y, new_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Is it enough of these two features to make predictions? \n",
    "# ## Neural Network with 2 features ('meanfun', 'IQR')\n",
    "# train_x = train_x[:,[5,12]]\n",
    "# test_x = test_x[:,[5,12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # It seems we can better result when using Keras deep learning library, \n",
    "# # so we will continue to use this package to do prediction.\n",
    "\n",
    "# #Initialisng the ANN\n",
    "# classifier = Sequential()\n",
    "\n",
    "# #Input Layer and First Hidden Layer\n",
    "# classifier.add(Dense(units = 512, activation = 'softmax', input_dim = train_x.shape[1]))\n",
    "\n",
    "# #Adding the Second hidden layer\n",
    "# classifier.add(Dense(units = 512, activation = 'softmax'))\n",
    "\n",
    "# # Adding the output layer\n",
    "# classifier.add(Dense(units = 2, activation = 'softmax'))\n",
    "\n",
    "# #Compiling the ANN\n",
    "# classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Fitting ANN to the training set\n",
    "# classifier.fit(train_x, train_y, batch_size = 68, epochs = 2000, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Predicting the Train set results\n",
    "# pred_y = classifier.predict(train_x)\n",
    "# pred_y = (pred_y > 0.5)\n",
    "# # pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels_tmp = []  \n",
    "# for label in pred_y:  \n",
    "#     tmp = []  \n",
    "#     if label[0] == True:  \n",
    "#         tmp = [1,0]  \n",
    "#     else:   \n",
    "#         tmp = [0,1]  \n",
    "#     labels_tmp.append(tmp)  \n",
    "# new_result = np.array(labels_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('In-sample accuracy in Neural Network using Keras package (with 2 features):%s' % (accuracy_score(train_y, new_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Predicting the Test set results\n",
    "# pred_y = classifier.predict(test_x)\n",
    "# pred_y = (pred_y > 0.5)\n",
    "# # pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels_tmp = []  \n",
    "# for label in pred_y:  \n",
    "#     tmp = []  \n",
    "#     if label[0] == True:  \n",
    "#         tmp = [1,0]  \n",
    "#     else:   \n",
    "#         tmp = [0,1]  \n",
    "#     labels_tmp.append(tmp)  \n",
    "# new_result = np.array(labels_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('Out-of-sample accuracy in Neural Network using Keras package (with 2 features):%s' % (accuracy_score(test_y, new_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "000\n",
    "In case with only 2 features(meanfun, IQR) without feature scaling results are more precise. \n",
    "\n",
    "Neural Network with only two features gives 87.0% against 83.7% with all features. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
