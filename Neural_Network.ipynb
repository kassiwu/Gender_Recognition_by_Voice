{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import random\n",
    "from sklearn.model_selection import train_test_split  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Installing Theano\n",
    "# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n",
    "\n",
    "# Installing Tensorflow\n",
    "# Install Tensorflow from the website: https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html\n",
    "\n",
    "# Installing Keras\n",
    "# pip install --upgrade keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -r pca_train_x\n",
    "%store -r pca_test_x\n",
    "%store -r pca13_train_x\n",
    "%store -r pca13_test_x\n",
    "\n",
    "%store -r train_x\n",
    "%store -r test_x\n",
    "%store -r train_y\n",
    "%store -r test_y\n",
    "\n",
    "%store -r train_x_two_features\n",
    "%store -r test_x_two_features\n",
    "%store -r train_y_two_features\n",
    "%store -r test_y_two_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2376/2376 [==============================] - 3s 1ms/step - loss: 0.5873 - acc: 0.7731\n",
      "Epoch 2/100\n",
      "2376/2376 [==============================] - 1s 256us/step - loss: 0.3322 - acc: 0.8927\n",
      "Epoch 3/100\n",
      "2376/2376 [==============================] - 1s 235us/step - loss: 0.1864 - acc: 0.9419\n",
      "Epoch 4/100\n",
      "2376/2376 [==============================] - 1s 271us/step - loss: 0.1192 - acc: 0.9613\n",
      "Epoch 5/100\n",
      "2376/2376 [==============================] - 1s 253us/step - loss: 0.0986 - acc: 0.9655\n",
      "Epoch 6/100\n",
      "2376/2376 [==============================] - 1s 252us/step - loss: 0.0886 - acc: 0.9710\n",
      "Epoch 7/100\n",
      "2376/2376 [==============================] - 1s 254us/step - loss: 0.0819 - acc: 0.9743\n",
      "Epoch 8/100\n",
      "2376/2376 [==============================] - 1s 321us/step - loss: 0.0779 - acc: 0.9760\n",
      "Epoch 9/100\n",
      "2376/2376 [==============================] - 1s 244us/step - loss: 0.0759 - acc: 0.9764\n",
      "Epoch 10/100\n",
      "2376/2376 [==============================] - 1s 266us/step - loss: 0.0737 - acc: 0.9773\n",
      "Epoch 11/100\n",
      "2376/2376 [==============================] - 1s 278us/step - loss: 0.0722 - acc: 0.9777\n",
      "Epoch 12/100\n",
      "2376/2376 [==============================] - 1s 325us/step - loss: 0.0706 - acc: 0.9785\n",
      "Epoch 13/100\n",
      "2376/2376 [==============================] - 1s 324us/step - loss: 0.0702 - acc: 0.9777\n",
      "Epoch 14/100\n",
      "2376/2376 [==============================] - 1s 307us/step - loss: 0.0685 - acc: 0.9777\n",
      "Epoch 15/100\n",
      "2376/2376 [==============================] - 1s 337us/step - loss: 0.0681 - acc: 0.9769\n",
      "Epoch 16/100\n",
      "2376/2376 [==============================] - 1s 354us/step - loss: 0.0671 - acc: 0.9785\n",
      "Epoch 17/100\n",
      "2376/2376 [==============================] - 1s 301us/step - loss: 0.0661 - acc: 0.9790\n",
      "Epoch 18/100\n",
      "2376/2376 [==============================] - 1s 308us/step - loss: 0.0655 - acc: 0.9794\n",
      "Epoch 19/100\n",
      "2376/2376 [==============================] - 1s 315us/step - loss: 0.0653 - acc: 0.9785\n",
      "Epoch 20/100\n",
      "2376/2376 [==============================] - 1s 323us/step - loss: 0.0651 - acc: 0.9794\n",
      "Epoch 21/100\n",
      "2376/2376 [==============================] - 1s 345us/step - loss: 0.0635 - acc: 0.9802\n",
      "Epoch 22/100\n",
      "2376/2376 [==============================] - 1s 309us/step - loss: 0.0636 - acc: 0.9806\n",
      "Epoch 23/100\n",
      "2376/2376 [==============================] - 1s 325us/step - loss: 0.0629 - acc: 0.9790\n",
      "Epoch 24/100\n",
      "2376/2376 [==============================] - 1s 334us/step - loss: 0.0624 - acc: 0.9798\n",
      "Epoch 25/100\n",
      "2376/2376 [==============================] - 1s 274us/step - loss: 0.0619 - acc: 0.9785\n",
      "Epoch 26/100\n",
      "2376/2376 [==============================] - 1s 292us/step - loss: 0.0611 - acc: 0.9802\n",
      "Epoch 27/100\n",
      "2376/2376 [==============================] - 1s 318us/step - loss: 0.0609 - acc: 0.9806\n",
      "Epoch 28/100\n",
      "2376/2376 [==============================] - 1s 288us/step - loss: 0.0604 - acc: 0.9815\n",
      "Epoch 29/100\n",
      "2376/2376 [==============================] - 1s 290us/step - loss: 0.0594 - acc: 0.9815\n",
      "Epoch 30/100\n",
      "2376/2376 [==============================] - 1s 283us/step - loss: 0.0597 - acc: 0.9802\n",
      "Epoch 31/100\n",
      "2376/2376 [==============================] - 1s 300us/step - loss: 0.0583 - acc: 0.9819\n",
      "Epoch 32/100\n",
      "2376/2376 [==============================] - 1s 316us/step - loss: 0.0584 - acc: 0.9798\n",
      "Epoch 33/100\n",
      "2376/2376 [==============================] - 1s 351us/step - loss: 0.0580 - acc: 0.9823\n",
      "Epoch 34/100\n",
      "2376/2376 [==============================] - 1s 333us/step - loss: 0.0573 - acc: 0.9806\n",
      "Epoch 35/100\n",
      "2376/2376 [==============================] - 1s 309us/step - loss: 0.0572 - acc: 0.9815\n",
      "Epoch 36/100\n",
      "2376/2376 [==============================] - 1s 307us/step - loss: 0.0566 - acc: 0.9811\n",
      "Epoch 37/100\n",
      "2376/2376 [==============================] - 1s 294us/step - loss: 0.0565 - acc: 0.9815\n",
      "Epoch 38/100\n",
      "2376/2376 [==============================] - 1s 321us/step - loss: 0.0553 - acc: 0.9827\n",
      "Epoch 39/100\n",
      "2376/2376 [==============================] - 1s 303us/step - loss: 0.0550 - acc: 0.9827\n",
      "Epoch 40/100\n",
      "2376/2376 [==============================] - 1s 284us/step - loss: 0.0551 - acc: 0.9819\n",
      "Epoch 41/100\n",
      "2376/2376 [==============================] - 1s 305us/step - loss: 0.0548 - acc: 0.9806\n",
      "Epoch 42/100\n",
      "2376/2376 [==============================] - 1s 300us/step - loss: 0.0545 - acc: 0.9815\n",
      "Epoch 43/100\n",
      "2376/2376 [==============================] - 1s 303us/step - loss: 0.0547 - acc: 0.9811\n",
      "Epoch 44/100\n",
      "2376/2376 [==============================] - 1s 305us/step - loss: 0.0533 - acc: 0.9819\n",
      "Epoch 45/100\n",
      "2376/2376 [==============================] - 1s 300us/step - loss: 0.0535 - acc: 0.9815\n",
      "Epoch 46/100\n",
      "2376/2376 [==============================] - 1s 292us/step - loss: 0.0534 - acc: 0.9823\n",
      "Epoch 47/100\n",
      "2376/2376 [==============================] - 1s 325us/step - loss: 0.0519 - acc: 0.9823\n",
      "Epoch 48/100\n",
      "2376/2376 [==============================] - 1s 334us/step - loss: 0.0524 - acc: 0.9823\n",
      "Epoch 49/100\n",
      "2376/2376 [==============================] - 1s 427us/step - loss: 0.0515 - acc: 0.9827\n",
      "Epoch 50/100\n",
      "2376/2376 [==============================] - 1s 336us/step - loss: 0.0519 - acc: 0.9819\n",
      "Epoch 51/100\n",
      "2376/2376 [==============================] - 1s 348us/step - loss: 0.0513 - acc: 0.9819\n",
      "Epoch 52/100\n",
      "2376/2376 [==============================] - 1s 329us/step - loss: 0.0509 - acc: 0.9832\n",
      "Epoch 53/100\n",
      "2376/2376 [==============================] - 1s 356us/step - loss: 0.0508 - acc: 0.9827\n",
      "Epoch 54/100\n",
      "2376/2376 [==============================] - 1s 318us/step - loss: 0.0498 - acc: 0.9823\n",
      "Epoch 55/100\n",
      "2376/2376 [==============================] - 1s 342us/step - loss: 0.0498 - acc: 0.9819\n",
      "Epoch 56/100\n",
      "2376/2376 [==============================] - 1s 342us/step - loss: 0.0488 - acc: 0.9811\n",
      "Epoch 57/100\n",
      "2376/2376 [==============================] - 1s 429us/step - loss: 0.0495 - acc: 0.9827\n",
      "Epoch 58/100\n",
      "2376/2376 [==============================] - 1s 370us/step - loss: 0.0486 - acc: 0.9823\n",
      "Epoch 59/100\n",
      "2376/2376 [==============================] - 1s 358us/step - loss: 0.0479 - acc: 0.9823\n",
      "Epoch 60/100\n",
      "2376/2376 [==============================] - 1s 298us/step - loss: 0.0480 - acc: 0.9832\n",
      "Epoch 61/100\n",
      "2376/2376 [==============================] - 1s 315us/step - loss: 0.0477 - acc: 0.9832\n",
      "Epoch 62/100\n",
      "2376/2376 [==============================] - 1s 319us/step - loss: 0.0475 - acc: 0.9844\n",
      "Epoch 63/100\n",
      "2376/2376 [==============================] - ETA: 0s - loss: 0.0467 - acc: 0.984 - 1s 302us/step - loss: 0.0466 - acc: 0.9836\n",
      "Epoch 64/100\n",
      "2376/2376 [==============================] - 1s 323us/step - loss: 0.0462 - acc: 0.9823\n",
      "Epoch 65/100\n",
      "2376/2376 [==============================] - 1s 348us/step - loss: 0.0456 - acc: 0.9840\n",
      "Epoch 66/100\n",
      "2376/2376 [==============================] - 1s 309us/step - loss: 0.0463 - acc: 0.9840\n",
      "Epoch 67/100\n",
      "2376/2376 [==============================] - 1s 295us/step - loss: 0.0453 - acc: 0.9840\n",
      "Epoch 68/100\n",
      "2376/2376 [==============================] - 1s 339us/step - loss: 0.0453 - acc: 0.9832\n",
      "Epoch 69/100\n",
      "2376/2376 [==============================] - 1s 377us/step - loss: 0.0443 - acc: 0.9836\n",
      "Epoch 70/100\n",
      "2376/2376 [==============================] - 1s 304us/step - loss: 0.0442 - acc: 0.9836\n",
      "Epoch 71/100\n",
      "2376/2376 [==============================] - 1s 364us/step - loss: 0.0443 - acc: 0.9848\n",
      "Epoch 72/100\n",
      "2376/2376 [==============================] - 1s 335us/step - loss: 0.0432 - acc: 0.9848\n",
      "Epoch 73/100\n",
      "2376/2376 [==============================] - 1s 329us/step - loss: 0.0436 - acc: 0.9836\n",
      "Epoch 74/100\n",
      "2376/2376 [==============================] - 1s 343us/step - loss: 0.0428 - acc: 0.9848\n",
      "Epoch 75/100\n",
      "2376/2376 [==============================] - 1s 316us/step - loss: 0.0427 - acc: 0.9853\n",
      "Epoch 76/100\n",
      "2376/2376 [==============================] - 1s 343us/step - loss: 0.0418 - acc: 0.9848\n",
      "Epoch 77/100\n",
      "2376/2376 [==============================] - 1s 292us/step - loss: 0.0423 - acc: 0.9844\n",
      "Epoch 78/100\n",
      "2376/2376 [==============================] - 1s 283us/step - loss: 0.0413 - acc: 0.9853\n",
      "Epoch 79/100\n",
      "2376/2376 [==============================] - 1s 353us/step - loss: 0.0414 - acc: 0.9840\n",
      "Epoch 80/100\n",
      "2376/2376 [==============================] - 1s 348us/step - loss: 0.0407 - acc: 0.9848\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2376/2376 [==============================] - 1s 298us/step - loss: 0.0404 - acc: 0.9853\n",
      "Epoch 82/100\n",
      "2376/2376 [==============================] - 1s 249us/step - loss: 0.0406 - acc: 0.9844\n",
      "Epoch 83/100\n",
      "2376/2376 [==============================] - 1s 267us/step - loss: 0.0401 - acc: 0.9857\n",
      "Epoch 84/100\n",
      "2376/2376 [==============================] - 1s 254us/step - loss: 0.0398 - acc: 0.9861\n",
      "Epoch 85/100\n",
      "2376/2376 [==============================] - 1s 243us/step - loss: 0.0396 - acc: 0.9857\n",
      "Epoch 86/100\n",
      "2376/2376 [==============================] - 1s 247us/step - loss: 0.0391 - acc: 0.9861\n",
      "Epoch 87/100\n",
      "2376/2376 [==============================] - 1s 293us/step - loss: 0.0389 - acc: 0.9857\n",
      "Epoch 88/100\n",
      "2376/2376 [==============================] - 1s 283us/step - loss: 0.0383 - acc: 0.9861\n",
      "Epoch 89/100\n",
      "2376/2376 [==============================] - 1s 266us/step - loss: 0.0388 - acc: 0.9861\n",
      "Epoch 90/100\n",
      "2376/2376 [==============================] - 1s 255us/step - loss: 0.0385 - acc: 0.9870\n",
      "Epoch 91/100\n",
      "2376/2376 [==============================] - 1s 275us/step - loss: 0.0384 - acc: 0.9861\n",
      "Epoch 92/100\n",
      "2376/2376 [==============================] - 1s 332us/step - loss: 0.0384 - acc: 0.9857\n",
      "Epoch 93/100\n",
      "2376/2376 [==============================] - 1s 298us/step - loss: 0.0376 - acc: 0.9853\n",
      "Epoch 94/100\n",
      "2376/2376 [==============================] - 1s 253us/step - loss: 0.0379 - acc: 0.9857\n",
      "Epoch 95/100\n",
      "2376/2376 [==============================] - 1s 271us/step - loss: 0.0369 - acc: 0.9870\n",
      "Epoch 96/100\n",
      "2376/2376 [==============================] - 1s 276us/step - loss: 0.0376 - acc: 0.9857\n",
      "Epoch 97/100\n",
      "2376/2376 [==============================] - 1s 268us/step - loss: 0.0376 - acc: 0.9861\n",
      "Epoch 98/100\n",
      "2376/2376 [==============================] - 1s 291us/step - loss: 0.0374 - acc: 0.9844\n",
      "Epoch 99/100\n",
      "2376/2376 [==============================] - 1s 272us/step - loss: 0.0363 - acc: 0.9870\n",
      "Epoch 100/100\n",
      "2376/2376 [==============================] - 1s 294us/step - loss: 0.0359 - acc: 0.9861\n",
      "In-sample accuracy in Neural Network using Keras package :0.9882154882154882\n",
      "Out-sample accuracy in Neural Network using Keras package :0.976010101010101\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1fff24a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2376/2376 [==============================] - 2s 892us/step - loss: 0.6029 - acc: 0.8451\n",
      "Epoch 2/100\n",
      "2376/2376 [==============================] - 1s 289us/step - loss: 0.2300 - acc: 0.9512\n",
      "Epoch 3/100\n",
      "2376/2376 [==============================] - 1s 272us/step - loss: 0.1342 - acc: 0.9596\n",
      "Epoch 4/100\n",
      "2376/2376 [==============================] - 1s 293us/step - loss: 0.1194 - acc: 0.9625\n",
      "Epoch 5/100\n",
      "2376/2376 [==============================] - ETA: 0s - loss: 0.1178 - acc: 0.964 - 1s 278us/step - loss: 0.1163 - acc: 0.9634\n",
      "Epoch 6/100\n",
      "2376/2376 [==============================] - 1s 303us/step - loss: 0.1156 - acc: 0.9630\n",
      "Epoch 7/100\n",
      "2376/2376 [==============================] - 1s 297us/step - loss: 0.1154 - acc: 0.9634\n",
      "Epoch 8/100\n",
      "2376/2376 [==============================] - 1s 339us/step - loss: 0.1154 - acc: 0.9621\n",
      "Epoch 9/100\n",
      "2376/2376 [==============================] - 1s 352us/step - loss: 0.1150 - acc: 0.9634\n",
      "Epoch 10/100\n",
      "2376/2376 [==============================] - 1s 332us/step - loss: 0.1144 - acc: 0.9625\n",
      "Epoch 11/100\n",
      "2376/2376 [==============================] - 1s 307us/step - loss: 0.1142 - acc: 0.9638\n",
      "Epoch 12/100\n",
      "2376/2376 [==============================] - 1s 345us/step - loss: 0.1133 - acc: 0.9651\n",
      "Epoch 13/100\n",
      "2376/2376 [==============================] - 1s 278us/step - loss: 0.1132 - acc: 0.9630\n",
      "Epoch 14/100\n",
      "2376/2376 [==============================] - 1s 256us/step - loss: 0.1124 - acc: 0.9659\n",
      "Epoch 15/100\n",
      "2376/2376 [==============================] - 1s 293us/step - loss: 0.1117 - acc: 0.9655\n",
      "Epoch 16/100\n",
      "2376/2376 [==============================] - 1s 314us/step - loss: 0.1106 - acc: 0.9646\n",
      "Epoch 17/100\n",
      "2376/2376 [==============================] - 1s 262us/step - loss: 0.1103 - acc: 0.9651\n",
      "Epoch 18/100\n",
      "2376/2376 [==============================] - 1s 308us/step - loss: 0.1092 - acc: 0.9655\n",
      "Epoch 19/100\n",
      "2376/2376 [==============================] - 1s 292us/step - loss: 0.1078 - acc: 0.9663\n",
      "Epoch 20/100\n",
      "2376/2376 [==============================] - 1s 287us/step - loss: 0.1064 - acc: 0.9659\n",
      "Epoch 21/100\n",
      "2376/2376 [==============================] - 1s 329us/step - loss: 0.1053 - acc: 0.9672\n",
      "Epoch 22/100\n",
      "2376/2376 [==============================] - 1s 281us/step - loss: 0.1038 - acc: 0.9676\n",
      "Epoch 23/100\n",
      "2376/2376 [==============================] - 1s 276us/step - loss: 0.1030 - acc: 0.9655\n",
      "Epoch 24/100\n",
      "2376/2376 [==============================] - 1s 408us/step - loss: 0.1018 - acc: 0.9659\n",
      "Epoch 25/100\n",
      "2376/2376 [==============================] - 1s 283us/step - loss: 0.1006 - acc: 0.9668\n",
      "Epoch 26/100\n",
      "2376/2376 [==============================] - 1s 457us/step - loss: 0.0999 - acc: 0.9659\n",
      "Epoch 27/100\n",
      "2376/2376 [==============================] - 1s 363us/step - loss: 0.0993 - acc: 0.9672\n",
      "Epoch 28/100\n",
      "2376/2376 [==============================] - 1s 395us/step - loss: 0.0987 - acc: 0.9668\n",
      "Epoch 29/100\n",
      "2376/2376 [==============================] - 1s 282us/step - loss: 0.0981 - acc: 0.9659\n",
      "Epoch 30/100\n",
      "2376/2376 [==============================] - 1s 308us/step - loss: 0.0977 - acc: 0.9672\n",
      "Epoch 31/100\n",
      "2376/2376 [==============================] - 1s 259us/step - loss: 0.0973 - acc: 0.9672\n",
      "Epoch 32/100\n",
      "2376/2376 [==============================] - 1s 278us/step - loss: 0.0969 - acc: 0.9663\n",
      "Epoch 33/100\n",
      "2376/2376 [==============================] - 1s 271us/step - loss: 0.0968 - acc: 0.9668\n",
      "Epoch 34/100\n",
      "2376/2376 [==============================] - 1s 271us/step - loss: 0.0964 - acc: 0.9655\n",
      "Epoch 35/100\n",
      "2376/2376 [==============================] - 1s 257us/step - loss: 0.0965 - acc: 0.9668\n",
      "Epoch 36/100\n",
      "2376/2376 [==============================] - 1s 255us/step - loss: 0.0960 - acc: 0.9663\n",
      "Epoch 37/100\n",
      "2376/2376 [==============================] - 1s 257us/step - loss: 0.0957 - acc: 0.9676\n",
      "Epoch 38/100\n",
      "2376/2376 [==============================] - 1s 274us/step - loss: 0.0956 - acc: 0.9655\n",
      "Epoch 39/100\n",
      "2376/2376 [==============================] - 1s 255us/step - loss: 0.0954 - acc: 0.9655\n",
      "Epoch 40/100\n",
      "2376/2376 [==============================] - 1s 272us/step - loss: 0.0952 - acc: 0.9663\n",
      "Epoch 41/100\n",
      "2376/2376 [==============================] - 1s 258us/step - loss: 0.0954 - acc: 0.9663\n",
      "Epoch 42/100\n",
      "2376/2376 [==============================] - 1s 265us/step - loss: 0.0946 - acc: 0.9668\n",
      "Epoch 43/100\n",
      "2376/2376 [==============================] - 1s 277us/step - loss: 0.0950 - acc: 0.9646\n",
      "Epoch 44/100\n",
      "2376/2376 [==============================] - 1s 276us/step - loss: 0.0943 - acc: 0.9659\n",
      "Epoch 45/100\n",
      "2376/2376 [==============================] - 1s 292us/step - loss: 0.0944 - acc: 0.9651\n",
      "Epoch 46/100\n",
      "2376/2376 [==============================] - 1s 289us/step - loss: 0.0943 - acc: 0.9659\n",
      "Epoch 47/100\n",
      "2376/2376 [==============================] - 1s 269us/step - loss: 0.0942 - acc: 0.9655\n",
      "Epoch 48/100\n",
      "2376/2376 [==============================] - 1s 276us/step - loss: 0.0941 - acc: 0.9651\n",
      "Epoch 49/100\n",
      "2376/2376 [==============================] - 1s 259us/step - loss: 0.0941 - acc: 0.9655\n",
      "Epoch 50/100\n",
      "2376/2376 [==============================] - 1s 283us/step - loss: 0.0943 - acc: 0.9655\n",
      "Epoch 51/100\n",
      "2376/2376 [==============================] - 1s 273us/step - loss: 0.0933 - acc: 0.9668\n",
      "Epoch 52/100\n",
      "2376/2376 [==============================] - 1s 282us/step - loss: 0.0940 - acc: 0.9659\n",
      "Epoch 53/100\n",
      "2376/2376 [==============================] - 1s 260us/step - loss: 0.0938 - acc: 0.9668\n",
      "Epoch 54/100\n",
      "2376/2376 [==============================] - 1s 268us/step - loss: 0.0933 - acc: 0.9659\n",
      "Epoch 55/100\n",
      "2376/2376 [==============================] - 1s 289us/step - loss: 0.0940 - acc: 0.9663\n",
      "Epoch 56/100\n",
      "2376/2376 [==============================] - 1s 279us/step - loss: 0.0934 - acc: 0.9672\n",
      "Epoch 57/100\n",
      "2376/2376 [==============================] - 1s 312us/step - loss: 0.0932 - acc: 0.9663\n",
      "Epoch 58/100\n",
      "2376/2376 [==============================] - 1s 292us/step - loss: 0.0932 - acc: 0.9668\n",
      "Epoch 59/100\n",
      "2376/2376 [==============================] - 1s 268us/step - loss: 0.0936 - acc: 0.9655\n",
      "Epoch 60/100\n",
      "2376/2376 [==============================] - 1s 267us/step - loss: 0.0930 - acc: 0.9663\n",
      "Epoch 61/100\n",
      "2376/2376 [==============================] - 1s 291us/step - loss: 0.0930 - acc: 0.9651\n",
      "Epoch 62/100\n",
      "2376/2376 [==============================] - 1s 265us/step - loss: 0.0932 - acc: 0.9659\n",
      "Epoch 63/100\n",
      "2376/2376 [==============================] - 1s 298us/step - loss: 0.0930 - acc: 0.9663\n",
      "Epoch 64/100\n",
      "2376/2376 [==============================] - 1s 289us/step - loss: 0.0929 - acc: 0.9663\n",
      "Epoch 65/100\n",
      "2376/2376 [==============================] - 1s 274us/step - loss: 0.0923 - acc: 0.9663\n",
      "Epoch 66/100\n",
      "2376/2376 [==============================] - 1s 337us/step - loss: 0.0926 - acc: 0.9672\n",
      "Epoch 67/100\n",
      "2376/2376 [==============================] - 1s 273us/step - loss: 0.0929 - acc: 0.9663\n",
      "Epoch 68/100\n",
      "2376/2376 [==============================] - 1s 268us/step - loss: 0.0923 - acc: 0.9659\n",
      "Epoch 69/100\n",
      "2376/2376 [==============================] - 1s 293us/step - loss: 0.0927 - acc: 0.9668\n",
      "Epoch 70/100\n",
      "2376/2376 [==============================] - 1s 272us/step - loss: 0.0924 - acc: 0.9668\n",
      "Epoch 71/100\n",
      "2376/2376 [==============================] - 1s 268us/step - loss: 0.0919 - acc: 0.9663\n",
      "Epoch 72/100\n",
      "2376/2376 [==============================] - 1s 265us/step - loss: 0.0923 - acc: 0.9672\n",
      "Epoch 73/100\n",
      "2376/2376 [==============================] - 1s 263us/step - loss: 0.0924 - acc: 0.9672\n",
      "Epoch 74/100\n",
      "2376/2376 [==============================] - 1s 271us/step - loss: 0.0921 - acc: 0.9655\n",
      "Epoch 75/100\n",
      "2376/2376 [==============================] - 1s 267us/step - loss: 0.0914 - acc: 0.9676\n",
      "Epoch 76/100\n",
      "2376/2376 [==============================] - 1s 285us/step - loss: 0.0914 - acc: 0.9680\n",
      "Epoch 77/100\n",
      "2376/2376 [==============================] - 1s 275us/step - loss: 0.0920 - acc: 0.9676\n",
      "Epoch 78/100\n",
      "2376/2376 [==============================] - 1s 272us/step - loss: 0.0916 - acc: 0.9659\n",
      "Epoch 79/100\n",
      "2376/2376 [==============================] - 1s 298us/step - loss: 0.0917 - acc: 0.9672\n",
      "Epoch 80/100\n",
      "2376/2376 [==============================] - 1s 310us/step - loss: 0.0917 - acc: 0.9680\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2376/2376 [==============================] - 1s 291us/step - loss: 0.0918 - acc: 0.9680\n",
      "Epoch 82/100\n",
      "2376/2376 [==============================] - 1s 276us/step - loss: 0.0907 - acc: 0.9680\n",
      "Epoch 83/100\n",
      "2376/2376 [==============================] - 1s 277us/step - loss: 0.0921 - acc: 0.9659\n",
      "Epoch 84/100\n",
      "2376/2376 [==============================] - 1s 295us/step - loss: 0.0913 - acc: 0.9684\n",
      "Epoch 85/100\n",
      "2376/2376 [==============================] - 1s 277us/step - loss: 0.0910 - acc: 0.9668\n",
      "Epoch 86/100\n",
      "2376/2376 [==============================] - 1s 284us/step - loss: 0.0914 - acc: 0.9676\n",
      "Epoch 87/100\n",
      "2376/2376 [==============================] - 1s 265us/step - loss: 0.0911 - acc: 0.9676\n",
      "Epoch 88/100\n",
      "2376/2376 [==============================] - ETA: 0s - loss: 0.0876 - acc: 0.969 - 1s 279us/step - loss: 0.0909 - acc: 0.9672\n",
      "Epoch 89/100\n",
      "2376/2376 [==============================] - 1s 238us/step - loss: 0.0913 - acc: 0.9672\n",
      "Epoch 90/100\n",
      "2376/2376 [==============================] - 1s 253us/step - loss: 0.0907 - acc: 0.9684\n",
      "Epoch 91/100\n",
      "2376/2376 [==============================] - 1s 232us/step - loss: 0.0908 - acc: 0.9668\n",
      "Epoch 92/100\n",
      "2376/2376 [==============================] - 1s 235us/step - loss: 0.0907 - acc: 0.9668\n",
      "Epoch 93/100\n",
      "2376/2376 [==============================] - 1s 251us/step - loss: 0.0907 - acc: 0.9663\n",
      "Epoch 94/100\n",
      "2376/2376 [==============================] - 1s 255us/step - loss: 0.0904 - acc: 0.9668\n",
      "Epoch 95/100\n",
      "2376/2376 [==============================] - 1s 245us/step - loss: 0.0910 - acc: 0.9676\n",
      "Epoch 96/100\n",
      "2376/2376 [==============================] - 1s 290us/step - loss: 0.0909 - acc: 0.9672\n",
      "Epoch 97/100\n",
      "2376/2376 [==============================] - 1s 333us/step - loss: 0.0908 - acc: 0.9672\n",
      "Epoch 98/100\n",
      "2376/2376 [==============================] - 1s 342us/step - loss: 0.0903 - acc: 0.9676\n",
      "Epoch 99/100\n",
      "2376/2376 [==============================] - 1s 290us/step - loss: 0.0904 - acc: 0.9663\n",
      "Epoch 100/100\n",
      "2376/2376 [==============================] - 1s 277us/step - loss: 0.0903 - acc: 0.9672\n",
      "In-sample accuracy in Neural Network using Keras package :0.9675925925925926\n",
      "Out-sample accuracy in Neural Network using Keras package :0.9747474747474747\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmVXlsDAzs6ocFmZmVpXDwszMqvr/s5NOo23fBf8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2088ac50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2376/2376 [==============================] - 2s 878us/step - loss: 0.5987 - acc: 0.8556\n",
      "Epoch 2/100\n",
      "2376/2376 [==============================] - 1s 288us/step - loss: 0.1937 - acc: 0.9575\n",
      "Epoch 3/100\n",
      "2376/2376 [==============================] - 1s 315us/step - loss: 0.1065 - acc: 0.9697\n",
      "Epoch 4/100\n",
      "2376/2376 [==============================] - 1s 310us/step - loss: 0.0904 - acc: 0.9760\n",
      "Epoch 5/100\n",
      "2376/2376 [==============================] - 1s 281us/step - loss: 0.0840 - acc: 0.9756\n",
      "Epoch 6/100\n",
      "2376/2376 [==============================] - 1s 326us/step - loss: 0.0809 - acc: 0.9769\n",
      "Epoch 7/100\n",
      "2376/2376 [==============================] - 1s 422us/step - loss: 0.0791 - acc: 0.9773\n",
      "Epoch 8/100\n",
      "2376/2376 [==============================] - 1s 317us/step - loss: 0.0783 - acc: 0.9794\n",
      "Epoch 9/100\n",
      "2376/2376 [==============================] - 1s 300us/step - loss: 0.0773 - acc: 0.9785\n",
      "Epoch 10/100\n",
      "2376/2376 [==============================] - 1s 319us/step - loss: 0.0767 - acc: 0.9798\n",
      "Epoch 11/100\n",
      "2376/2376 [==============================] - 1s 273us/step - loss: 0.0758 - acc: 0.9798\n",
      "Epoch 12/100\n",
      "2376/2376 [==============================] - 1s 318us/step - loss: 0.0750 - acc: 0.9785\n",
      "Epoch 13/100\n",
      "2376/2376 [==============================] - 1s 424us/step - loss: 0.0752 - acc: 0.9794\n",
      "Epoch 14/100\n",
      "2376/2376 [==============================] - 1s 320us/step - loss: 0.0742 - acc: 0.9790\n",
      "Epoch 15/100\n",
      "2376/2376 [==============================] - 1s 422us/step - loss: 0.0747 - acc: 0.9794 0s - loss: 0.0660 - \n",
      "Epoch 16/100\n",
      "2376/2376 [==============================] - 1s 402us/step - loss: 0.0737 - acc: 0.9798\n",
      "Epoch 17/100\n",
      "2376/2376 [==============================] - 1s 339us/step - loss: 0.0734 - acc: 0.9794\n",
      "Epoch 18/100\n",
      "2376/2376 [==============================] - 1s 318us/step - loss: 0.0732 - acc: 0.9781\n",
      "Epoch 19/100\n",
      "2376/2376 [==============================] - 1s 322us/step - loss: 0.0727 - acc: 0.9785\n",
      "Epoch 20/100\n",
      "2376/2376 [==============================] - 1s 341us/step - loss: 0.0724 - acc: 0.9798\n",
      "Epoch 21/100\n",
      "2376/2376 [==============================] - 1s 464us/step - loss: 0.0716 - acc: 0.9785\n",
      "Epoch 22/100\n",
      "2376/2376 [==============================] - 1s 321us/step - loss: 0.0716 - acc: 0.9794\n",
      "Epoch 23/100\n",
      "2376/2376 [==============================] - 1s 294us/step - loss: 0.0713 - acc: 0.9790\n",
      "Epoch 24/100\n",
      "2376/2376 [==============================] - 1s 283us/step - loss: 0.0712 - acc: 0.9790\n",
      "Epoch 25/100\n",
      "2376/2376 [==============================] - 1s 364us/step - loss: 0.0711 - acc: 0.9798\n",
      "Epoch 26/100\n",
      "2376/2376 [==============================] - 1s 281us/step - loss: 0.0705 - acc: 0.9781\n",
      "Epoch 27/100\n",
      "2376/2376 [==============================] - 1s 280us/step - loss: 0.0702 - acc: 0.9798\n",
      "Epoch 28/100\n",
      "2376/2376 [==============================] - 1s 304us/step - loss: 0.0699 - acc: 0.9794\n",
      "Epoch 29/100\n",
      "2376/2376 [==============================] - 1s 284us/step - loss: 0.0705 - acc: 0.9785\n",
      "Epoch 30/100\n",
      "2376/2376 [==============================] - 1s 322us/step - loss: 0.0697 - acc: 0.9794\n",
      "Epoch 31/100\n",
      "2376/2376 [==============================] - 1s 316us/step - loss: 0.0695 - acc: 0.9798\n",
      "Epoch 32/100\n",
      "2376/2376 [==============================] - 1s 305us/step - loss: 0.0692 - acc: 0.9794\n",
      "Epoch 33/100\n",
      "2376/2376 [==============================] - 1s 294us/step - loss: 0.0690 - acc: 0.9798\n",
      "Epoch 34/100\n",
      "2376/2376 [==============================] - 1s 281us/step - loss: 0.0689 - acc: 0.9794\n",
      "Epoch 35/100\n",
      "2376/2376 [==============================] - 1s 287us/step - loss: 0.0690 - acc: 0.9785\n",
      "Epoch 36/100\n",
      "2376/2376 [==============================] - 1s 293us/step - loss: 0.0688 - acc: 0.9794\n",
      "Epoch 37/100\n",
      "2376/2376 [==============================] - 1s 345us/step - loss: 0.0686 - acc: 0.9798\n",
      "Epoch 38/100\n",
      "2376/2376 [==============================] - 1s 297us/step - loss: 0.0687 - acc: 0.9781\n",
      "Epoch 39/100\n",
      "2376/2376 [==============================] - 1s 275us/step - loss: 0.0686 - acc: 0.9798\n",
      "Epoch 40/100\n",
      "2376/2376 [==============================] - 1s 243us/step - loss: 0.0685 - acc: 0.9790\n",
      "Epoch 41/100\n",
      "2376/2376 [==============================] - 1s 294us/step - loss: 0.0683 - acc: 0.9790\n",
      "Epoch 42/100\n",
      "2376/2376 [==============================] - 1s 253us/step - loss: 0.0684 - acc: 0.9798\n",
      "Epoch 43/100\n",
      "2376/2376 [==============================] - 1s 245us/step - loss: 0.0686 - acc: 0.9781\n",
      "Epoch 44/100\n",
      "2376/2376 [==============================] - 1s 238us/step - loss: 0.0688 - acc: 0.9781\n",
      "Epoch 45/100\n",
      "2376/2376 [==============================] - 1s 269us/step - loss: 0.0681 - acc: 0.9802\n",
      "Epoch 46/100\n",
      "2376/2376 [==============================] - 1s 287us/step - loss: 0.0677 - acc: 0.9798\n",
      "Epoch 47/100\n",
      "2376/2376 [==============================] - 1s 263us/step - loss: 0.0679 - acc: 0.9777\n",
      "Epoch 48/100\n",
      "2376/2376 [==============================] - 1s 291us/step - loss: 0.0679 - acc: 0.9790\n",
      "Epoch 49/100\n",
      "2376/2376 [==============================] - 1s 237us/step - loss: 0.0680 - acc: 0.9802\n",
      "Epoch 50/100\n",
      "2376/2376 [==============================] - 1s 282us/step - loss: 0.0679 - acc: 0.9785\n",
      "Epoch 51/100\n",
      "2376/2376 [==============================] - 1s 281us/step - loss: 0.0676 - acc: 0.9794\n",
      "Epoch 52/100\n",
      "2376/2376 [==============================] - 1s 243us/step - loss: 0.0677 - acc: 0.9785\n",
      "Epoch 53/100\n",
      "2376/2376 [==============================] - 1s 252us/step - loss: 0.0673 - acc: 0.9798\n",
      "Epoch 54/100\n",
      "2376/2376 [==============================] - 1s 246us/step - loss: 0.0673 - acc: 0.9790\n",
      "Epoch 55/100\n",
      "2376/2376 [==============================] - 1s 242us/step - loss: 0.0675 - acc: 0.9794\n",
      "Epoch 56/100\n",
      "2376/2376 [==============================] - 1s 249us/step - loss: 0.0671 - acc: 0.9802\n",
      "Epoch 57/100\n",
      "2376/2376 [==============================] - 1s 244us/step - loss: 0.0668 - acc: 0.9798\n",
      "Epoch 58/100\n",
      "2376/2376 [==============================] - 1s 271us/step - loss: 0.0670 - acc: 0.9790\n",
      "Epoch 59/100\n",
      "2376/2376 [==============================] - 1s 269us/step - loss: 0.0674 - acc: 0.9802\n",
      "Epoch 60/100\n",
      "2376/2376 [==============================] - 1s 298us/step - loss: 0.0672 - acc: 0.9798\n",
      "Epoch 61/100\n",
      "2376/2376 [==============================] - 1s 292us/step - loss: 0.0672 - acc: 0.9798\n",
      "Epoch 62/100\n",
      "2376/2376 [==============================] - 1s 277us/step - loss: 0.0666 - acc: 0.9798\n",
      "Epoch 63/100\n",
      "2376/2376 [==============================] - 1s 269us/step - loss: 0.0670 - acc: 0.9790\n",
      "Epoch 64/100\n",
      "2376/2376 [==============================] - 1s 252us/step - loss: 0.0663 - acc: 0.9806\n",
      "Epoch 65/100\n",
      "2376/2376 [==============================] - 1s 267us/step - loss: 0.0664 - acc: 0.9794\n",
      "Epoch 66/100\n",
      "2376/2376 [==============================] - 1s 253us/step - loss: 0.0665 - acc: 0.9790\n",
      "Epoch 67/100\n",
      "2376/2376 [==============================] - 1s 269us/step - loss: 0.0666 - acc: 0.9798\n",
      "Epoch 68/100\n",
      "2376/2376 [==============================] - 1s 290us/step - loss: 0.0662 - acc: 0.9806\n",
      "Epoch 69/100\n",
      "2376/2376 [==============================] - 1s 277us/step - loss: 0.0661 - acc: 0.9802\n",
      "Epoch 70/100\n",
      "2376/2376 [==============================] - 1s 277us/step - loss: 0.0657 - acc: 0.9785\n",
      "Epoch 71/100\n",
      "2376/2376 [==============================] - 1s 294us/step - loss: 0.0659 - acc: 0.9785\n",
      "Epoch 72/100\n",
      "2376/2376 [==============================] - 1s 249us/step - loss: 0.0655 - acc: 0.9790\n",
      "Epoch 73/100\n",
      "2376/2376 [==============================] - 1s 273us/step - loss: 0.0657 - acc: 0.9802\n",
      "Epoch 74/100\n",
      "2376/2376 [==============================] - 1s 296us/step - loss: 0.0656 - acc: 0.9802\n",
      "Epoch 75/100\n",
      "2376/2376 [==============================] - 1s 240us/step - loss: 0.0650 - acc: 0.9798\n",
      "Epoch 76/100\n",
      "2376/2376 [==============================] - 1s 253us/step - loss: 0.0655 - acc: 0.9802\n",
      "Epoch 77/100\n",
      "2376/2376 [==============================] - 1s 284us/step - loss: 0.0653 - acc: 0.9802\n",
      "Epoch 78/100\n",
      "2376/2376 [==============================] - 1s 352us/step - loss: 0.0650 - acc: 0.9794\n",
      "Epoch 79/100\n",
      "2376/2376 [==============================] - 1s 268us/step - loss: 0.0652 - acc: 0.9802\n",
      "Epoch 80/100\n",
      "2376/2376 [==============================] - 1s 413us/step - loss: 0.0649 - acc: 0.9790\n",
      "Epoch 81/100\n",
      "2376/2376 [==============================] - 1s 385us/step - loss: 0.0650 - acc: 0.9790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      "2376/2376 [==============================] - 1s 373us/step - loss: 0.0647 - acc: 0.9798\n",
      "Epoch 83/100\n",
      "2376/2376 [==============================] - 1s 303us/step - loss: 0.0650 - acc: 0.9811\n",
      "Epoch 84/100\n",
      "2376/2376 [==============================] - 1s 313us/step - loss: 0.0649 - acc: 0.9806\n",
      "Epoch 85/100\n",
      "2376/2376 [==============================] - 1s 288us/step - loss: 0.0651 - acc: 0.9785\n",
      "Epoch 86/100\n",
      "2376/2376 [==============================] - 1s 504us/step - loss: 0.0648 - acc: 0.9806\n",
      "Epoch 87/100\n",
      "2376/2376 [==============================] - 1s 386us/step - loss: 0.0645 - acc: 0.9794\n",
      "Epoch 88/100\n",
      "2376/2376 [==============================] - 1s 325us/step - loss: 0.0640 - acc: 0.9806\n",
      "Epoch 89/100\n",
      "2376/2376 [==============================] - 1s 329us/step - loss: 0.0645 - acc: 0.9806\n",
      "Epoch 90/100\n",
      "2376/2376 [==============================] - 1s 302us/step - loss: 0.0644 - acc: 0.9806\n",
      "Epoch 91/100\n",
      "2376/2376 [==============================] - 1s 365us/step - loss: 0.0647 - acc: 0.9798\n",
      "Epoch 92/100\n",
      "2376/2376 [==============================] - 1s 377us/step - loss: 0.0641 - acc: 0.9785\n",
      "Epoch 93/100\n",
      "2376/2376 [==============================] - 1s 382us/step - loss: 0.0645 - acc: 0.9806\n",
      "Epoch 94/100\n",
      "2376/2376 [==============================] - 1s 323us/step - loss: 0.0636 - acc: 0.9785\n",
      "Epoch 95/100\n",
      "2376/2376 [==============================] - 1s 325us/step - loss: 0.0643 - acc: 0.9802\n",
      "Epoch 96/100\n",
      "2376/2376 [==============================] - 1s 334us/step - loss: 0.0642 - acc: 0.9794\n",
      "Epoch 97/100\n",
      "2376/2376 [==============================] - 1s 374us/step - loss: 0.0639 - acc: 0.9806\n",
      "Epoch 98/100\n",
      "2376/2376 [==============================] - 1s 358us/step - loss: 0.0638 - acc: 0.9798\n",
      "Epoch 99/100\n",
      "2376/2376 [==============================] - 1s 354us/step - loss: 0.0643 - acc: 0.9802\n",
      "Epoch 100/100\n",
      "2376/2376 [==============================] - 1s 346us/step - loss: 0.0638 - acc: 0.9811\n",
      "In-sample accuracy in Neural Network using Keras package :0.9797979797979798\n",
      "Out-sample accuracy in Neural Network using Keras package :0.9696969696969697\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZlaVw8LMzKpyWJiZWVX/B4eOUUkzSLCUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a20df68d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import the keras lib and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import sklearn.metrics as metrics\n",
    "from keras.callbacks import History \n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def run_ANN(train_x,test_x,train_y,test_y):\n",
    "    #Initializing ANN\n",
    "    clf = Sequential()\n",
    "    hist = History()\n",
    "    clf.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = len(train_x[1])))\n",
    "    # Adding the second hidden layer\n",
    "    clf.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "    # Adding the output layer\n",
    "    clf.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "    clf.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    clf.fit(train_x, train_y, batch_size = 10, nb_epoch = 100 ,callbacks = [hist])\n",
    "    pred_y = clf.predict(test_x)\n",
    "    pred_y = (pred_y > 0.5)\n",
    "    \n",
    "    pred_y_t = clf.predict(train_x)\n",
    "    pred_y_t = (pred_y_t > 0.5)\n",
    "    \n",
    "    print('In-sample accuracy in Neural Network using Keras package :%s' % (metrics.accuracy_score(pred_y_t,train_y)))\n",
    "    print('Out-sample accuracy in Neural Network using Keras package :%s' % metrics.accuracy_score(pred_y,test_y))  \n",
    "#     print(metrics.accuracy_score(pred_y,test_y))\n",
    "#     print(metrics.accuracy_score(pred_y_t,train_y))\n",
    "\n",
    "    plt.plot(hist.history['acc'], color = 'red')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "        \n",
    "run_ANN(train_x,test_x,train_y,test_y)\n",
    "run_ANN(train_x_two_features, test_x_two_features, train_y_two_features, test_y_two_features)\n",
    "run_ANN(pca13_train_x, pca13_test_x, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Data preparation\n",
    "# path = '/Users/Kassi/Desktop/Gender_Recognition_by_Voice/voice.csv'\n",
    "# voice_data = pd.read_csv(path)\n",
    "# voice_data = voice_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # There are multiple steps we will take in this section to transform the original data into format which we can easily plug inside tensorflow's tensors.\n",
    "# # Creating 1-hot vector from the original labels\n",
    "# # Randomly shuffle data\n",
    "# # Create train/test datasets\n",
    "\n",
    "# voices = voice_data[:, :-1] \n",
    "# labels = voice_data[:, -1:]\n",
    "\n",
    "# # Classification: label(gender) = {male, female}  [1.0, 0.0], [0.0, 1.0]  \n",
    "# labels_tmp = []  \n",
    "# for label in labels:  \n",
    "#     tmp = []  \n",
    "#     if label[0] == 'male':  \n",
    "#         tmp = [1.0, 0.0]  \n",
    "#     else:   \n",
    "#         tmp = [0.0, 1.0]  \n",
    "#     labels_tmp.append(tmp)  \n",
    "# labels = np.array(labels_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # randomly shuffle data\n",
    "# voices_tmp = []  \n",
    "# lables_tmp = []  \n",
    "# index_shuf = range(len(voices)) \n",
    "# random.shuffle(index_shuf) \n",
    "# for i in index_shuf:  \n",
    "#     voices_tmp.append(voices[i])  \n",
    "#     lables_tmp.append(labels[i])  \n",
    "# voices = np.array(voices_tmp)  \n",
    "# labels = np.array(lables_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_x, test_x, train_y, test_y = train_test_split(voices, labels, test_size=0.2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we are all set to get started with creating the tensorflow graph. \n",
    "# It is important to understand that after this step there is no actual computation being done by tensorflow. \n",
    "# It just creates a lazy graph according to the nodes we create in the neural_network method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Learning Parameters\n",
    "# rate   =   0.0010  # training rate\n",
    "# epochs =  2000     # number of full training cycles (I select this number based on the trend of cost)\n",
    "# banch_size  = 68   # number of data points to train per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Network Parameters\n",
    "# n_hidden_1 = 512  # number of nodes in hidden layer 1\n",
    "# n_hidden_2 = 512  # number of nodes in hidden layer 2\n",
    "# n_input    = voices.shape[-1]  # 20\n",
    "# n_classes  = 2\n",
    "# n_samples  = len(voices)  # 460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = tf.placeholder('float', [None,n_input])\n",
    "# Y = tf.placeholder('float', [None,n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# weights = {\n",
    "#     'w1' : tf.Variable(tf.random_normal([n_input,    n_hidden_1])),\n",
    "#     'w2' : tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "#     'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes ]))\n",
    "# }\n",
    "\n",
    "# biases = {\n",
    "#     'b1' : tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "#     'b2' : tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "#     'out': tf.Variable(tf.random_normal([n_classes ]))\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def neural_network(X, weights, biases):  \n",
    "#     # Hidden Layer 1\n",
    "#     layer1 = tf.matmul(X, weights['w1'])\n",
    "#     layer1 = tf.add(layer1, biases['b1'])\n",
    "#     layer1 = tf.nn.softmax(layer1)\n",
    "    \n",
    "#     # Hidden Layer 2\n",
    "#     layer2 = tf.matmul(layer1, weights['w2'])\n",
    "#     layer2 = tf.add(layer2, biases['b2'])\n",
    "#     layer2 = tf.nn.softmax(layer2)\n",
    "    \n",
    "#     # Output Layer\n",
    "#     output = tf.matmul(layer2, weights['out'])\n",
    "#     output = tf.add(output, biases['out'])\n",
    "#     output = tf.nn.softmax(output)\n",
    "    \n",
    "#     return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # In this section we define the optimizer and loss functions. \n",
    "# # We also define our notion of correct and incorrect prediction and what we mean by accuracy.\n",
    "\n",
    "# def train_neural_network(train_x,train_y,test_x,test_y):\n",
    "#     model = neural_network(X, weights, biases)\n",
    "#     f_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "#     f_optimizer = tf.train.AdamOptimizer(learning_rate=rate).minimize(f_cost)\n",
    "#     with tf.Session() as s:\n",
    "#         s.run(tf.global_variables_initializer())\n",
    "#         for epoch in range(epochs):\n",
    "#             cost_avg = 0.\n",
    "#             batch_total = len(train_x) // banch_size\n",
    "#             for banch in range(batch_total):\n",
    "#                 voice_banch = train_x[banch*banch_size:(banch+1)*(banch_size)]  \n",
    "#                 label_banch = train_y[banch*banch_size:(banch+1)*(banch_size)]        \n",
    "#                 _, cost = s.run([f_optimizer,f_cost], feed_dict={X:voice_banch, Y: label_banch})        \n",
    "#                 cost_avg += cost / batch_total\n",
    "        \n",
    "#             print('Epoch {}: cost={:.4f}'.format(epoch+1, cost_avg))\n",
    "    \n",
    "#         # testing\n",
    "        \n",
    "#         # This gives us a list of booleans\n",
    "#         prediction = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))  \n",
    "#         # We cast to floating point numbers and then take the mean\n",
    "#         accuracy = tf.reduce_mean(tf.cast(prediction, dtype=tf.float32))  \n",
    "#         accuracy = s.run(accuracy, feed_dict={X: train_x, Y: train_y})  \n",
    "#         print('In-sample accuracy in Neural Network: %s'  % (accuracy)) \n",
    "        \n",
    "#         accuracy = tf.reduce_mean(tf.cast(prediction, dtype=tf.float32)) \n",
    "#         accuracy = s.run(accuracy, feed_dict={X: test_x, Y: test_y})  \n",
    "#         print('Out-of-sample accuracy in Neural Network: %s'  % (accuracy)) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Execute tf graph\n",
    "# # Now that the graph is set up, we can run it\n",
    "\n",
    "# print('Accuracy in Neural Network with all features:')\n",
    "# train_neural_network(train_x,train_y,test_x,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are wondering if there is a big difference in the results when used Keras deep learning library with a TensorFlow backend vs. our own step-by-step algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Importing Libraries for ANN\n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# #Initialisng the ANN\n",
    "# classifier = Sequential()\n",
    "\n",
    "# #Input Layer and First Hidden Layer\n",
    "# classifier.add(Dense(units = 512, activation = 'softmax', input_dim = train_x.shape[1]))\n",
    "\n",
    "# #Adding the Second hidden layer\n",
    "# classifier.add(Dense(units = 512, activation = 'softmax'))\n",
    "\n",
    "# # Adding the output layer\n",
    "# classifier.add(Dense(units = 2, activation = 'softmax'))\n",
    "\n",
    "# #Compiling the ANN\n",
    "# classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Fitting ANN to the training set\n",
    "# classifier.fit(train_x, train_y, batch_size = 68, epochs = 2000, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Predicting the Test set results\n",
    "# pred_train_y = classifier.predict(train_x)\n",
    "# pred_train_y = (pred_train_y > 0.5)\n",
    "# # pred_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels_tmp = []  \n",
    "# for label in pred_train_y:  \n",
    "#     tmp = []  \n",
    "#     if label[0] == True:  \n",
    "#         tmp = [1,0]  \n",
    "#     else:   \n",
    "#         tmp = [0,1]  \n",
    "#     labels_tmp.append(tmp)  \n",
    "# new_result = np.array(labels_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# print('In-sample accuracy in Neural Network using Keras package (with all features):%s' % (accuracy_score(train_y, new_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Predicting the Test set results\n",
    "# pred_test_y = classifier.predict(test_x)\n",
    "# pred_test_y = (pred_test_y > 0.5)\n",
    "# # pred_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels_tmp = []  \n",
    "# for label in pred_test_y:  \n",
    "#     tmp = []  \n",
    "#     if label[0] == True:  \n",
    "#         tmp = [1,0]  \n",
    "#     else:   \n",
    "#         tmp = [0,1]  \n",
    "#     labels_tmp.append(tmp)  \n",
    "# new_result = np.array(labels_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Out-of-sample accuracy in Neural Network using Keras package (with all features):%s' % (accuracy_score(test_y, new_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Is it enough of these two features to make predictions? \n",
    "# ## Neural Network with 2 features ('meanfun', 'IQR')\n",
    "# train_x = train_x[:,[5,12]]\n",
    "# test_x = test_x[:,[5,12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # It seems we can better result when using Keras deep learning library, \n",
    "# # so we will continue to use this package to do prediction.\n",
    "\n",
    "# #Initialisng the ANN\n",
    "# classifier = Sequential()\n",
    "\n",
    "# #Input Layer and First Hidden Layer\n",
    "# classifier.add(Dense(units = 512, activation = 'softmax', input_dim = train_x.shape[1]))\n",
    "\n",
    "# #Adding the Second hidden layer\n",
    "# classifier.add(Dense(units = 512, activation = 'softmax'))\n",
    "\n",
    "# # Adding the output layer\n",
    "# classifier.add(Dense(units = 2, activation = 'softmax'))\n",
    "\n",
    "# #Compiling the ANN\n",
    "# classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Fitting ANN to the training set\n",
    "# classifier.fit(train_x, train_y, batch_size = 68, epochs = 2000, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Predicting the Train set results\n",
    "# pred_y = classifier.predict(train_x)\n",
    "# pred_y = (pred_y > 0.5)\n",
    "# # pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels_tmp = []  \n",
    "# for label in pred_y:  \n",
    "#     tmp = []  \n",
    "#     if label[0] == True:  \n",
    "#         tmp = [1,0]  \n",
    "#     else:   \n",
    "#         tmp = [0,1]  \n",
    "#     labels_tmp.append(tmp)  \n",
    "# new_result = np.array(labels_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('In-sample accuracy in Neural Network using Keras package (with 2 features):%s' % (accuracy_score(train_y, new_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Predicting the Test set results\n",
    "# pred_y = classifier.predict(test_x)\n",
    "# pred_y = (pred_y > 0.5)\n",
    "# # pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels_tmp = []  \n",
    "# for label in pred_y:  \n",
    "#     tmp = []  \n",
    "#     if label[0] == True:  \n",
    "#         tmp = [1,0]  \n",
    "#     else:   \n",
    "#         tmp = [0,1]  \n",
    "#     labels_tmp.append(tmp)  \n",
    "# new_result = np.array(labels_tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('Out-of-sample accuracy in Neural Network using Keras package (with 2 features):%s' % (accuracy_score(test_y, new_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "000\n",
    "In case with only 2 features(meanfun, IQR) without feature scaling results are more precise. \n",
    "\n",
    "Neural Network with only two features gives 87.0% against 83.7% with all features. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
